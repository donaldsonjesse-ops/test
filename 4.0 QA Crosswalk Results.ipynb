{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 4.0 QA Crosswalk Results\n",
    "\n",
    "**Quality Assurance notebook for crosswalk outputs.**\n",
    "\n",
    "## Workflow\n",
    "1. Upload crosswalk results file\n",
    "2. Clean data (capitalization, remove numbering)\n",
    "3. Save cleaned file\n",
    "4. Run QA classification (HIGH/MEDIUM/LOW confidence)\n",
    "5. Export final QA'd results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas openpyxl tqdm -q\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Dependencies loaded.\")\n",
    "print(\"Ready to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Upload Crosswalk File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"UPLOAD CROSSWALK RESULTS FILE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUpload your crosswalk Excel file (e.g., 3.03_All_Province_*.xlsx)\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "INPUT_FILE = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Loaded: {INPUT_FILE}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nRows: {len(df)}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAMPLE DATA (first 3 rows):\")\n",
    "print(\"=\"*70)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CLEANING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text: remove numbering prefixes, consistent capitalization.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Remove leading numbering like \"7.\", \"8.\", \"9.\", \"10.\", \"11.\" etc.\n",
    "    text = re.sub(r'^\\d+\\.\\s*', '', text.strip())\n",
    "    \n",
    "    # Remove leading numbering with letters like \"7a.\", \"10b.\" etc.\n",
    "    text = re.sub(r'^\\d+[a-z]?\\.\\s*', '', text.strip())\n",
    "    \n",
    "    # Title case for descriptions (capitalize first letter of each sentence)\n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_section_name(text):\n",
    "    \"\"\"Clean section names: remove leading numbers, title case.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Remove leading numbering like \"7.\", \"8.\", \"9.\", \"10.\" etc.\n",
    "    text = re.sub(r'^\\d+\\.\\s*', '', text.strip())\n",
    "    \n",
    "    # Remove leading numbering with letters\n",
    "    text = re.sub(r'^\\d+[a-z]?\\.\\s*', '', text.strip())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Store original for comparison\n",
    "df_original = df.copy()\n",
    "\n",
    "# Clean Description column\n",
    "if 'Description' in df.columns:\n",
    "    df['Description'] = df['Description'].apply(clean_text)\n",
    "    print(\"  - Cleaned: Description\")\n",
    "\n",
    "# Clean AB_Description column\n",
    "if 'AB_Description' in df.columns:\n",
    "    df['AB_Description'] = df['AB_Description'].apply(clean_text)\n",
    "    print(\"  - Cleaned: AB_Description\")\n",
    "\n",
    "# Clean section columns\n",
    "section_cols = ['Level_1_Section', 'Level_2_Subsection', 'Level_3_Heading', 'Specialty']\n",
    "for col in section_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_section_name)\n",
    "        print(f\"  - Cleaned: {col}\")\n",
    "\n",
    "# Clean Reasoning column\n",
    "if 'Reasoning' in df.columns:\n",
    "    df['Reasoning'] = df['Reasoning'].apply(clean_text)\n",
    "    print(\"  - Cleaned: Reasoning\")\n",
    "\n",
    "# Clean additional_notes column\n",
    "if 'additional_notes' in df.columns:\n",
    "    df['additional_notes'] = df['additional_notes'].apply(clean_text)\n",
    "    print(\"  - Cleaned: additional_notes\")\n",
    "\n",
    "# Standardize Type column to uppercase\n",
    "if 'Type' in df.columns:\n",
    "    df['Type'] = df['Type'].str.upper()\n",
    "    print(\"  - Standardized: Type (uppercase)\")\n",
    "\n",
    "# Standardize Modality column to lowercase\n",
    "if 'Modality' in df.columns:\n",
    "    df['Modality'] = df['Modality'].str.lower()\n",
    "    print(\"  - Standardized: Modality (lowercase)\")\n",
    "\n",
    "# Standardize Target_Province to uppercase\n",
    "if 'Target_Province' in df.columns:\n",
    "    df['Target_Province'] = df['Target_Province'].str.upper()\n",
    "    print(\"  - Standardized: Target_Province (uppercase)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLEANING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRows: {len(df)}\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(f\"\\nSample cleaned data:\")\n",
    "df[['Code', 'Description', 'Fee', 'Type']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Save Cleaned File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cleaned filename\n",
    "cleaned_filename = INPUT_FILE.replace('.xlsx', '_CLEANED.xlsx')\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(cleaned_filename, index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLEANED FILE SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSaved: {cleaned_filename}\")\n",
    "print(f\"Rows: {len(df)}\")\n",
    "\n",
    "# Download cleaned file\n",
    "files.download(cleaned_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"API KEY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "OPENAI_API_KEY = \"\"  # <-- Paste your key here, or leave blank to use getpass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    from getpass import getpass\n",
    "    OPENAI_API_KEY = getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"\\nâœ“ API client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: QA Classification\n",
    "\n",
    "Classify each code match as **HIGH**, **MEDIUM**, or **LOW** confidence crossover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"QA CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cost tracking\n",
    "total_cost = 0.0\n",
    "total_calls = 0\n",
    "\n",
    "def track_cost(inp_tokens, out_tokens):\n",
    "    global total_cost, total_calls\n",
    "    total_cost += (inp_tokens/1e6)*3.0 + (out_tokens/1e6)*15.0\n",
    "    total_calls += 1\n",
    "\n",
    "def build_qa_prompt(rows_batch, ab_code, ab_description):\n",
    "    \"\"\"Build QA prompt for a batch of rows.\"\"\"\n",
    "    \n",
    "    rows_text = \"\"\n",
    "    for idx, row in rows_batch.iterrows():\n",
    "        rows_text += f\"\"\"\n",
    "ROW {idx}:\n",
    "- Target Province: {row.get('Target_Province', 'N/A')}\n",
    "- Code: {row.get('Code', 'N/A')}\n",
    "- Description: {row.get('Description', 'N/A')}\n",
    "- Fee: {row.get('Fee', 'N/A')}\n",
    "- Type: {row.get('Type', 'N/A')}\n",
    "- Modality: {row.get('Modality', 'N/A')}\n",
    "- Reasoning: {row.get('Reasoning', 'N/A')}\n",
    "\"\"\"\n",
    "    \n",
    "    return f\"\"\"You are a senior physician billing specialist performing quality assurance on a billing code crosswalk.\n",
    "\n",
    "ALBERTA SOURCE CODE:\n",
    "- Code: {ab_code}\n",
    "- Description: {ab_description}\n",
    "\n",
    "TARGET CODES TO EVALUATE:\n",
    "{rows_text}\n",
    "\n",
    "TASK:\n",
    "For each row, evaluate how well the target code matches the Alberta source code and assign a confidence level:\n",
    "\n",
    "- **HIGH**: Direct equivalent - same clinical service, similar fee structure, clear match\n",
    "- **MEDIUM**: Partial match - related service but different scope, conditions, or fee structure\n",
    "- **LOW**: Weak match - tangentially related, likely not a true crosswalk equivalent\n",
    "\n",
    "IMPORTANT: Consider:\n",
    "1. Clinical equivalence (same procedure/service?)\n",
    "2. Fee reasonableness (similar value?)\n",
    "3. Modality match (if applicable)\n",
    "4. Any conditions or restrictions that affect equivalence\n",
    "\n",
    "Return JSON array with one object per row:\n",
    "[\n",
    "  {{\n",
    "    \"row_index\": <integer>,\n",
    "    \"qa_confidence\": \"HIGH|MEDIUM|LOW\",\n",
    "    \"qa_rationale\": \"Brief explanation (1-2 sentences)\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "JSON only:\"\"\"\n",
    "\n",
    "def run_qa_batch(rows_batch, ab_code, ab_description):\n",
    "    \"\"\"Run QA on a batch of rows.\"\"\"\n",
    "    prompt = build_qa_prompt(rows_batch, ab_code, ab_description)\n",
    "    \n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-5.1-2025-11-13\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=2000\n",
    "        )\n",
    "        track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n",
    "        \n",
    "        content = resp.choices[0].message.content\n",
    "        \n",
    "        # Extract JSON array\n",
    "        match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group())\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get Alberta code info\n",
    "ab_code = df['AB_Code'].iloc[0] if 'AB_Code' in df.columns else 'Unknown'\n",
    "ab_description = df['AB_Description'].iloc[0] if 'AB_Description' in df.columns else 'Unknown'\n",
    "\n",
    "print(f\"\\nAlberta Source Code: {ab_code}\")\n",
    "print(f\"Description: {ab_description}\")\n",
    "print(f\"\\nTotal rows to QA: {len(df)}\")\n",
    "print(f\"Batch size: 5 rows per API call\")\n",
    "print(f\"Estimated API calls: {(len(df) + 4) // 5}\")\n",
    "\n",
    "# Initialize QA columns\n",
    "df['QA_Confidence'] = ''\n",
    "df['QA_Rationale'] = ''\n",
    "\n",
    "# Process in batches of 5\n",
    "batch_size = 5\n",
    "total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RUNNING QA...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for batch_num in tqdm(range(total_batches), desc=\"QA Batches\"):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(df))\n",
    "    \n",
    "    batch_df = df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    results = run_qa_batch(batch_df, ab_code, ab_description)\n",
    "    \n",
    "    # Map results back to dataframe\n",
    "    for result in results:\n",
    "        row_idx = result.get('row_index')\n",
    "        if row_idx is not None and start_idx <= row_idx < end_idx:\n",
    "            df.at[row_idx, 'QA_Confidence'] = result.get('qa_confidence', '')\n",
    "            df.at[row_idx, 'QA_Rationale'] = result.get('qa_rationale', '')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"QA COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nAPI calls: {total_calls}\")\n",
    "print(f\"Estimated cost: ${total_cost:.2f}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- QA SUMMARY ---\")\n",
    "print(df['QA_Confidence'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL QA RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reorder columns to put QA columns prominently\n",
    "qa_cols = ['QA_Confidence', 'QA_Rationale']\n",
    "key_cols = ['AB_Code', 'Target_Province', 'Code', 'Description', 'Fee', 'Type', 'Modality']\n",
    "other_cols = [c for c in df.columns if c not in qa_cols + key_cols]\n",
    "\n",
    "# New column order: QA first, then key info, then rest\n",
    "new_order = [c for c in key_cols if c in df.columns] + qa_cols + [c for c in other_cols if c in df.columns]\n",
    "df_final = df[new_order]\n",
    "\n",
    "# Sort by QA_Confidence (HIGH first, then MEDIUM, then LOW)\n",
    "confidence_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2, '': 3}\n",
    "df_final['_sort'] = df_final['QA_Confidence'].map(confidence_order)\n",
    "df_final = df_final.sort_values(['Target_Province', '_sort', 'Code']).drop(columns=['_sort'])\n",
    "\n",
    "# Save final file\n",
    "final_filename = INPUT_FILE.replace('.xlsx', '_QA_COMPLETE.xlsx')\n",
    "df_final.to_excel(final_filename, index=False)\n",
    "\n",
    "print(f\"\\nSaved: {final_filename}\")\n",
    "print(f\"Total rows: {len(df_final)}\")\n",
    "\n",
    "print(f\"\\n--- BY CONFIDENCE ---\")\n",
    "for conf in ['HIGH', 'MEDIUM', 'LOW']:\n",
    "    count = len(df_final[df_final['QA_Confidence'] == conf])\n",
    "    pct = count / len(df_final) * 100 if len(df_final) > 0 else 0\n",
    "    print(f\"  {conf}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- BY PROVINCE ---\")\n",
    "for prov in df_final['Target_Province'].unique():\n",
    "    prov_df = df_final[df_final['Target_Province'] == prov]\n",
    "    high = len(prov_df[prov_df['QA_Confidence'] == 'HIGH'])\n",
    "    med = len(prov_df[prov_df['QA_Confidence'] == 'MEDIUM'])\n",
    "    low = len(prov_df[prov_df['QA_Confidence'] == 'LOW'])\n",
    "    print(f\"  {prov}: {len(prov_df)} total (H:{high} M:{med} L:{low})\")\n",
    "\n",
    "# Download final file\n",
    "files.download(final_filename)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"QA WORKFLOW COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Display Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final table\n",
    "display_cols = ['Target_Province', 'Code', 'Description', 'Fee', 'QA_Confidence', 'QA_Rationale']\n",
    "display_cols = [c for c in display_cols if c in df_final.columns]\n",
    "\n",
    "print(\"FINAL QA TABLE:\")\n",
    "print()\n",
    "df_final[display_cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
