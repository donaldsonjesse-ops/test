{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba Section Pilot - Crosswalk by Level 1 Sections\n",
    "\n",
    "**Approach:** Process PDF by logical Level 1 sections (specialties) from reference CSV rather than arbitrary page chunks.\n",
    "\n",
    "1. Split PDF into Level 1 section chunks using page ranges from CSV\n",
    "2. LLM reads entire section chunk and extracts all telehealth codes\n",
    "3. Extract Rules of Application (pages 1-82) separately\n",
    "4. Phase 2: Use stored chunks + rules to extract attributes\n",
    "\n",
    "**Benefits:**\n",
    "- Each specialty section processed as coherent whole\n",
    "- No arbitrary splitting mid-section\n",
    "- LLM sees full context for each specialty\n",
    "- Dynamic token limits based on section size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas pdfplumber openpyxl tqdm PyMuPDF -q\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Upload Manitoba PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Upload Manitoba Payment Schedule PDF:\")\n",
    "print(\"(MB Payment Schedule - April 1, 2024.pdf)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "MB_PDF = None\n",
    "for f in uploaded.keys():\n",
    "    MB_PDF = f\n",
    "    break\n",
    "\n",
    "if MB_PDF:\n",
    "    print(f\"\\nLoaded: {MB_PDF}\")\n",
    "else:\n",
    "    print(\"ERROR: No file uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Upload Section Reference CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload manitoba_section_reference_final.csv:\")\n",
    "uploaded_ref = files.upload()\n",
    "\n",
    "section_ref_file = list(uploaded_ref.keys())[0]\n",
    "df_section_ref = pd.read_csv(section_ref_file)\n",
    "\n",
    "# Sort by page_start\n",
    "df_section_ref = df_section_ref.sort_values('page_start').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nLoaded {len(df_section_ref)} section entries\")\n",
    "print(f\"Page range: {df_section_ref['page_start'].min()} - {df_section_ref['page_start'].max()}\")\n",
    "print(f\"\\nUnique Level 1 sections (specialties): {df_section_ref['level_1'].nunique()}\")\n",
    "print(f\"\\nLevel 1 sections:\")\n",
    "for l1 in df_section_ref['level_1'].unique():\n",
    "    min_page = df_section_ref[df_section_ref['level_1'] == l1]['page_start'].min()\n",
    "    print(f\"  Page {min_page:3}: {l1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # <-- Paste your key here\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    from getpass import getpass\n",
    "    OPENAI_API_KEY = getpass(\"API Key: \")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "print(\"API ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Split PDF into Level 1 Section Chunks\n",
    "\n",
    "Extract text for each Level 1 section (specialty) based on page ranges from CSV.\n",
    "Skip pages 1-82 (Rules of Application) - these are extracted separately for Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Loading PDF and splitting into Level 1 sections...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First, load all pages\n",
    "pdf_pages = {}\n",
    "with pdfplumber.open(MB_PDF) as pdf:\n",
    "    total_pages = len(pdf.pages)\n",
    "    for i, page in enumerate(tqdm(pdf.pages, desc=\"Loading pages\")):\n",
    "        page_num = i + 1\n",
    "        try:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                pdf_pages[page_num] = text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"\\nLoaded {len(pdf_pages)} pages from PDF (total: {total_pages})\")\n",
    "\n",
    "# Build section chunks using Level 1 page ranges\n",
    "# Get unique Level 1 sections with their minimum page_start\n",
    "level_1_sections = df_section_ref.groupby('level_1')['page_start'].min().sort_values()\n",
    "\n",
    "# Sections to skip (preamble, appendices)\n",
    "SKIP_SECTIONS = [\n",
    "    \"APPENDICES\",\n",
    "]\n",
    "\n",
    "section_chunks = {}\n",
    "level_1_list = list(level_1_sections.items())\n",
    "\n",
    "for idx, (section_name, start_page) in enumerate(level_1_list):\n",
    "    # Skip appendices\n",
    "    if section_name in SKIP_SECTIONS:\n",
    "        continue\n",
    "    \n",
    "    # Skip Rules of Application (pages 1-82)\n",
    "    if start_page < 83:\n",
    "        continue\n",
    "    \n",
    "    # End page is start of next section - 1, or last page of PDF\n",
    "    if idx + 1 < len(level_1_list):\n",
    "        end_page = level_1_list[idx + 1][1] - 1\n",
    "    else:\n",
    "        end_page = total_pages\n",
    "    \n",
    "    # Extract text for this section\n",
    "    section_text = \"\"\n",
    "    pages_in_section = []\n",
    "    for pg in range(start_page, end_page + 1):\n",
    "        if pg in pdf_pages:\n",
    "            section_text += f\"\\n=== PAGE {pg} ===\\n{pdf_pages[pg]}\"\n",
    "            pages_in_section.append(pg)\n",
    "    \n",
    "    section_chunks[section_name] = {\n",
    "        'text': section_text,\n",
    "        'start_page': start_page,\n",
    "        'end_page': end_page,\n",
    "        'page_count': len(pages_in_section),\n",
    "        'char_count': len(section_text)\n",
    "    }\n",
    "\n",
    "print(f\"\\nCreated {len(section_chunks)} section chunks:\")\n",
    "print(\"-\"*70)\n",
    "for name, info in section_chunks.items():\n",
    "    print(f\"  {name[:50]:50} | Pages {info['start_page']:3}-{info['end_page']:3} ({info['page_count']:2} pgs) | {info['char_count']:,} chars\")\n",
    "\n",
    "print(\"\\nSection chunks ready for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Alberta Code Definition + Prompt Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Alberta code definition\n",
    "AB_CODE = \"03.03CV\"\n",
    "AB_DESC = \"Telehealth consultation\"\n",
    "AB_FEE = 25.09\n",
    "\n",
    "AB_CLINICAL_DEFINITION = \"\"\"Assessment of a patient's condition via telephone or secure videoconference.\n",
    "\n",
    "NOTE:\n",
    "- At minimum: limited assessment requiring history related to presenting problems, appropriate records review, and advice to the patient\n",
    "- Total physician time spent providing patient care must be MINIMUM 10 MINUTES\n",
    "- If less than 10 minutes same day, must use HSC 03.01AD instead\n",
    "- May only be claimed if service was initiated by the patient or their agent\n",
    "- May only be claimed if service is personally rendered by the physician\n",
    "- Benefit includes ordering appropriate diagnostic tests and discussion with patient\n",
    "- Patient record must include detailed summary of all services including start/stop times\n",
    "- Time spent on administrative tasks cannot be claimed\n",
    "- May NOT be claimed same day as: 03.01AD, 03.01S, 03.01T, 03.03FV, 03.05JR, 03.08CV, 08.19CV, 08.19CW, or 08.19CX by same physician for same patient\n",
    "- May NOT be claimed same day as in-person visit or consultation by same physician for same patient\n",
    "\n",
    "Category: V Visit (Virtual)\n",
    "Base rate: $25.09\"\"\"\n",
    "\n",
    "# Tracking\n",
    "total_cost = 0.0\n",
    "total_calls = 0\n",
    "\n",
    "def track_cost(inp, out):\n",
    "    global total_cost, total_calls\n",
    "    total_cost += (inp/1e6)*3.0 + (out/1e6)*15.0\n",
    "    total_calls += 1\n",
    "\n",
    "def build_section_prompt(section_name, section_info):\n",
    "    \"\"\"Build prompt for processing a complete Level 1 section.\"\"\"\n",
    "    section_text = section_info['text']\n",
    "    start_page = section_info['start_page']\n",
    "    end_page = section_info['end_page']\n",
    "    \n",
    "    return f\"\"\"You are a senior physician billing specialist mapping Alberta fee codes to Manitoba equivalents.\n",
    "\n",
    "ALBERTA CODE TO MATCH:\n",
    "- Code: {AB_CODE}\n",
    "- Description: {AB_DESC}\n",
    "- Fee: ${AB_FEE}\n",
    "\n",
    "CLINICAL SERVICE DEFINITION:\n",
    "{AB_CLINICAL_DEFINITION}\n",
    "\n",
    "This is a BASIC PATIENT-FACING virtual visit by any physician (not specialist-specific, not physician-to-physician).\n",
    "\n",
    "You are reviewing the COMPLETE section for: {section_name}\n",
    "Pages {start_page} to {end_page}\n",
    "\n",
    "MANITOBA PAYMENT SCHEDULE - FULL SECTION:\n",
    "\n",
    "{section_text}\n",
    "\n",
    "TASK:\n",
    "Find ALL Manitoba codes in this section that bill for patient-facing virtual assessments (telephone or video consultations with patients).\n",
    "\n",
    "ACCURACY RULES - YOU MUST FOLLOW:\n",
    "\n",
    "1. **ONLY REAL CODES**: Return ONLY codes that LITERALLY appear in the text above.\n",
    "   - Copy the EXACT code as shown (e.g., 8321, 8340, 8447)\n",
    "   - If you cannot find the exact code string in the text, DO NOT include it\n",
    "   - NEVER invent, fabricate, or guess codes\n",
    "\n",
    "2. **EXACT VALUES**: Copy fee EXACTLY as shown in the document\n",
    "   - Use exact decimal values (e.g., \"59.05\" not \"59.00\")\n",
    "   - If fee is percentage-based premium, use \"-\" and explain in condition\n",
    "\n",
    "3. **FULL DESCRIPTIONS - CLIENT READY FORMAT**:\n",
    "   - Copy the COMPLETE service description as written in the schedule\n",
    "   - Do NOT abbreviate (write \"Virtual visit by telephone or video\" not \"Virtual visit\")\n",
    "   - Do NOT truncate (include the full description text)\n",
    "   - Use sentence case for consistency (capitalize first word and proper nouns)\n",
    "   - Include qualifying details (e.g., \"minimum 10 minutes\")\n",
    "   - Format: Clear, professional, ready for client delivery\n",
    "\n",
    "4. **MODALITY**: Only include modalities explicitly stated\n",
    "   - \"telephone\" = text says telephone/phone only\n",
    "   - \"video\" = text says video/videoconference only\n",
    "   - \"both\" = text explicitly allows BOTH, or doesn't restrict\n",
    "\n",
    "5. **PAGE NUMBERS**: page_found must match the \"=== PAGE X ===\" marker where code appears\n",
    "\n",
    "6. **SECTION HEADING**: Extract the subsection heading the code appears under\n",
    "   - Look for bold/uppercase headings like \"VIRTUAL VISITS\", \"HOSPITAL CARE\", etc.\n",
    "   - This becomes level_2_subsection\n",
    "\n",
    "WHAT TO LOOK FOR:\n",
    "- Virtual visits\n",
    "- Telephone consultations / assessments\n",
    "- Video consultations / assessments\n",
    "- Telehealth codes\n",
    "- Any code that can be billed for a patient-facing virtual encounter\n",
    "\n",
    "DO NOT INCLUDE:\n",
    "- Physician-to-physician consultations (e-consults between doctors)\n",
    "- E-assessments / e-consults (specialist-to-PCP) - not patient-facing\n",
    "- In-person only codes\n",
    "- Diagnostic procedures (ECG, imaging, labs)\n",
    "- Codes you cannot find literally in the text\n",
    "\n",
    "JSON only:\n",
    "{{\n",
    "  \"section_name\": \"{section_name}\",\n",
    "  \"found\": true/false,\n",
    "  \"primary_codes\": [\n",
    "    {{\n",
    "      \"code\": \"EXACT code (e.g., 8321, 8340)\",\n",
    "      \"description\": \"COMPLETE description in sentence case - do not abbreviate or truncate\",\n",
    "      \"fee\": \"EXACT fee or '-' for percentage-based\",\n",
    "      \"modality\": \"telephone|video|both\",\n",
    "      \"page_found\": <integer>,\n",
    "      \"section_heading\": \"subsection heading this code appears under (e.g., VIRTUAL VISITS)\",\n",
    "      \"reasoning\": \"brief explanation why this matches\"\n",
    "    }}\n",
    "  ],\n",
    "  \"add_on_codes\": [\n",
    "    {{\n",
    "      \"code\": \"EXACT code\",\n",
    "      \"description\": \"COMPLETE description in sentence case\",\n",
    "      \"fee\": \"EXACT fee or '-'\",\n",
    "      \"modality\": \"telephone|video|both\",\n",
    "      \"page_found\": <integer>,\n",
    "      \"section_heading\": \"subsection heading\",\n",
    "      \"links_to\": [\"codes this add-on applies to - only if explicitly stated\"],\n",
    "      \"condition\": \"when this add-on applies (include percentage if applicable)\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "If no telehealth/virtual codes in this section: {{\"section_name\": \"{section_name}\", \"found\": false, \"primary_codes\": [], \"add_on_codes\": []}}\"\"\"\n",
    "\n",
    "print(f\"Alberta Code: {AB_CODE} - {AB_DESC} (${AB_FEE})\")\n",
    "print(\"Prompt builder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Phase 1 - Process Each Level 1 Section\n",
    "\n",
    "One API call per section - LLM sees the complete section context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Process each Level 1 section\n",
    "\n",
    "prov_code = \"MB\"\n",
    "prov_name = \"Manitoba\"\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"PHASE 1: PROCESSING BY LEVEL 1 SECTIONS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "all_results = []\n",
    "code_chunks = {}  # Store section text for Phase 2\n",
    "\n",
    "def get_dynamic_max_tokens(char_count):\n",
    "    \"\"\"Set max_completion_tokens based on section size.\"\"\"\n",
    "    if char_count > 150000:\n",
    "        return 20000\n",
    "    elif char_count > 80000:\n",
    "        return 14000\n",
    "    elif char_count > 40000:\n",
    "        return 10000\n",
    "    elif char_count > 15000:\n",
    "        return 6000\n",
    "    else:\n",
    "        return 4000\n",
    "\n",
    "for section_name, section_info in tqdm(section_chunks.items(), desc=\"Processing sections\"):\n",
    "    \n",
    "    char_count = section_info['char_count']\n",
    "    max_tokens = get_dynamic_max_tokens(char_count)\n",
    "    \n",
    "    print(f\"\\n[{section_name[:50]}]\")\n",
    "    print(f\"  Pages {section_info['start_page']}-{section_info['end_page']} | {char_count:,} chars | {max_tokens} max tokens\")\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = build_section_prompt(section_name, section_info)\n",
    "    \n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-5.1-2025-11-13\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=max_tokens\n",
    "        )\n",
    "        track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n",
    "        \n",
    "        content = resp.choices[0].message.content\n",
    "        match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
    "        \n",
    "        if match:\n",
    "            result = json.loads(match.group())\n",
    "            \n",
    "            n_primary = len(result.get('primary_codes', []))\n",
    "            n_addon = len(result.get('add_on_codes', []))\n",
    "            \n",
    "            if result.get('found'):\n",
    "                print(f\"  -> Found {n_primary} primary, {n_addon} add-ons\")\n",
    "                \n",
    "                # Process primary codes\n",
    "                for p in result.get('primary_codes', []):\n",
    "                    code = p.get('code', '')\n",
    "                    fee = str(p.get('fee', ''))\n",
    "                    modality = p.get('modality', '')\n",
    "                    \n",
    "                    # Unique key for dedup and Phase 2\n",
    "                    unique_key = f\"{code}_{fee}_{modality}_{section_name}\"\n",
    "                    code_chunks[unique_key] = section_info['text']\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'AB_Code': AB_CODE,\n",
    "                        'AB_Description': AB_DESC,\n",
    "                        'AB_Fee': AB_FEE,\n",
    "                        'Target_Province': prov_code,\n",
    "                        'Code': code,\n",
    "                        'Description': p.get('description', ''),\n",
    "                        'Fee': p.get('fee', ''),\n",
    "                        'Type': 'PRIMARY',\n",
    "                        'Modality': modality,\n",
    "                        'Specialty': section_name,\n",
    "                        'Links_To': '',\n",
    "                        'Condition': '',\n",
    "                        'Reasoning': p.get('reasoning', ''),\n",
    "                        'Level_1_Section': section_name,\n",
    "                        'Level_2_Subsection': p.get('section_heading', ''),\n",
    "                        'Page_Found': p.get('page_found', ''),\n",
    "                        '_unique_key': unique_key\n",
    "                    })\n",
    "                \n",
    "                # Process add-on codes\n",
    "                for a in result.get('add_on_codes', []):\n",
    "                    code = a.get('code', '')\n",
    "                    fee = str(a.get('fee', ''))\n",
    "                    modality = a.get('modality', '')\n",
    "                    \n",
    "                    unique_key = f\"{code}_{fee}_{modality}_{section_name}\"\n",
    "                    code_chunks[unique_key] = section_info['text']\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'AB_Code': AB_CODE,\n",
    "                        'AB_Description': AB_DESC,\n",
    "                        'AB_Fee': AB_FEE,\n",
    "                        'Target_Province': prov_code,\n",
    "                        'Code': code,\n",
    "                        'Description': a.get('description', ''),\n",
    "                        'Fee': a.get('fee', ''),\n",
    "                        'Type': 'ADD-ON',\n",
    "                        'Modality': modality,\n",
    "                        'Specialty': section_name,\n",
    "                        'Links_To': ', '.join(a.get('links_to', [])) if a.get('links_to') else '',\n",
    "                        'Condition': a.get('condition', ''),\n",
    "                        'Reasoning': '',\n",
    "                        'Level_1_Section': section_name,\n",
    "                        'Level_2_Subsection': a.get('section_heading', ''),\n",
    "                        'Page_Found': a.get('page_found', ''),\n",
    "                        '_unique_key': unique_key\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"  -> No telehealth codes found\")\n",
    "        else:\n",
    "            print(f\"  -> ERROR: Could not parse JSON response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  -> ERROR: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 1 COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total codes found: {len(all_results)}\")\n",
    "print(f\"  - PRIMARY: {sum(1 for r in all_results if r['Type'] == 'PRIMARY')}\")\n",
    "print(f\"  - ADD-ON: {sum(1 for r in all_results if r['Type'] == 'ADD-ON')}\")\n",
    "print(f\"API calls: {total_calls} | Cost: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Phase 1 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Phase 1 results by section\n",
    "import pandas as pd\n",
    "\n",
    "df_phase1 = pd.DataFrame(all_results)\n",
    "\n",
    "if len(df_phase1) > 0:\n",
    "    print(\"PHASE 1 RESULTS BY SECTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    section_counts = df_phase1.groupby('Level_1_Section').size().sort_index()\n",
    "    for section, count in section_counts.items():\n",
    "        print(f\"  {section[:50]:50} | {count:3} codes\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ALL CODES FOUND:\")\n",
    "    print(\"-\"*70)\n",
    "    for _, row in df_phase1.iterrows():\n",
    "        print(f\"  {row['Code']:8} | ${str(row['Fee']):>7} | {row['Type']:8} | {row['Modality']:10} | pg {row['Page_Found']:>3} | {row['Level_1_Section'][:25]}\")\n",
    "else:\n",
    "    print(\"No codes found in Phase 1\")\n",
    "\n",
    "# Save Phase 1\n",
    "phase1_file = 'phase1_mb_section_pilot.xlsx'\n",
    "df_phase1.to_excel(phase1_file, index=False)\n",
    "print(f\"\\nSaved: {phase1_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: Attribute Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Extract Rules of Application (Pages 1-82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Extract Rules of Application pages for Manitoba (pages 1-82)\n",
    "RULES_START_PAGE = 1\n",
    "RULES_END_PAGE = 82\n",
    "\n",
    "print(f\"Extracting Rules of Application (pages {RULES_START_PAGE}-{RULES_END_PAGE})...\")\n",
    "\n",
    "# Open source PDF\n",
    "src_pdf = fitz.open(MB_PDF)\n",
    "\n",
    "# Create new PDF with rules pages\n",
    "rules_pdf = fitz.open()\n",
    "rules_pdf.insert_pdf(src_pdf, from_page=RULES_START_PAGE-1, to_page=RULES_END_PAGE-1)\n",
    "\n",
    "# Save Rules PDF\n",
    "rules_pdf_file = 'mb_rules_of_application.pdf'\n",
    "rules_pdf.save(rules_pdf_file)\n",
    "print(f\"Saved: {rules_pdf_file} ({RULES_END_PAGE - RULES_START_PAGE + 1} pages)\")\n",
    "\n",
    "# Extract text from Rules for use in prompts\n",
    "rules_of_application_text = \"\"\n",
    "for page_num in range(RULES_START_PAGE - 1, RULES_END_PAGE):\n",
    "    page = src_pdf[page_num]\n",
    "    text = page.get_text()\n",
    "    if text:\n",
    "        rules_of_application_text += f\"\\n=== RULES PAGE {page_num + 1} ===\\n{text}\"\n",
    "\n",
    "src_pdf.close()\n",
    "rules_pdf.close()\n",
    "\n",
    "print(f\"Loaded Rules of Application text: {len(rules_of_application_text):,} characters\")\n",
    "\n",
    "# Download the Rules PDF\n",
    "files.download(rules_pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Load Extraction Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extraction taxonomy\n",
    "print(\"Upload extraction_taxonomy.xlsx:\")\n",
    "uploaded_tax = files.upload()\n",
    "\n",
    "taxonomy_file = list(uploaded_tax.keys())[0]\n",
    "df_taxonomy = pd.read_excel(taxonomy_file)\n",
    "\n",
    "print(f\"\\nLoaded {len(df_taxonomy)} attributes:\")\n",
    "for _, row in df_taxonomy.iterrows():\n",
    "    print(f\"  - {row['attribute']}: {row['data_type']}\")\n",
    "\n",
    "# Build taxonomy reference string for prompts\n",
    "taxonomy_reference = \"\\n\".join([\n",
    "    f\"- {row['attribute']} ({row['data_type']}): {row['definition']} Taxonomy: {row['taxonomy']}\"\n",
    "    for _, row in df_taxonomy.iterrows()\n",
    "])\n",
    "\n",
    "print(\"\\nTaxonomy loaded and ready for Phase 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Phase 2 - Extract Attributes for Each Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Extract attributes for each code using stored section chunks + rules\n",
    "\n",
    "def build_attribute_prompt(code_info, chunk_text, rules_text, taxonomy_ref):\n",
    "    \"\"\"Build prompt to extract attributes for a single code.\"\"\"\n",
    "    return f\"\"\"You are a senior physician billing specialist extracting detailed attributes for a Manitoba billing code.\n",
    "\n",
    "CODE TO ANALYZE:\n",
    "- Code: {code_info['Code']}\n",
    "- Description: {code_info['Description']}\n",
    "- Fee: {code_info['Fee']}\n",
    "- Type: {code_info['Type']}\n",
    "- Specialty/Section: {code_info.get('Specialty', 'N/A')}\n",
    "- Condition (from Phase 1): {code_info.get('Condition', 'N/A')}\n",
    "\n",
    "ATTRIBUTES TO EXTRACT:\n",
    "{taxonomy_ref}\n",
    "\n",
    "RULES OF APPLICATION (Pages 1-82 - general billing rules):\n",
    "{rules_text[:50000]}\n",
    "\n",
    "CODE-SPECIFIC SECTION (where this code was found):\n",
    "{chunk_text[:30000]}\n",
    "\n",
    "TASK:\n",
    "Using ALL available information above, extract values for each attribute.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Use information from BOTH the Rules of Application AND the code-specific section\n",
    "2. For each attribute, extract the value if found, or null if not stated\n",
    "3. For same_day_exclusions: return as array of code strings\n",
    "4. For additional_notes: ONLY include important billing information not captured elsewhere\n",
    "\n",
    "Return JSON only:\n",
    "{{\n",
    "  \"modality\": \"telephone|video|both|in_person|asynchronous|null\",\n",
    "  \"minimum_time_minutes\": integer or null,\n",
    "  \"frequency_per_day\": integer or null,\n",
    "  \"frequency_per_year\": integer or null,\n",
    "  \"frequency_per_year_period\": \"annual|quarterly|90_days|monthly|null\",\n",
    "  \"same_day_exclusions\": [\"code1\", \"code2\"] or [] or null,\n",
    "  \"premium_extended_hours\": \"rate% code conditions\" or null,\n",
    "  \"premium_location\": \"rate% code conditions\" or null,\n",
    "  \"premium_age\": \"rate% conditions\" or null,\n",
    "  \"premium_other\": \"rate% code conditions\" or null,\n",
    "  \"additional_notes\": \"other important billing info\" or null\n",
    "}}\"\"\"\n",
    "\n",
    "# Process each code\n",
    "phase2_results = []\n",
    "\n",
    "print(f\"Extracting attributes for {len(all_results)} codes...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, code_info in enumerate(tqdm(all_results, desc=\"Extracting attributes\")):\n",
    "    unique_key = code_info.get('_unique_key', '')\n",
    "    chunk_text = code_chunks.get(unique_key, '')\n",
    "    \n",
    "    prompt = build_attribute_prompt(code_info, chunk_text, rules_of_application_text, taxonomy_reference)\n",
    "    \n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-5.1-2025-11-13\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=1500\n",
    "        )\n",
    "        track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n",
    "        \n",
    "        content = resp.choices[0].message.content\n",
    "        match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
    "        \n",
    "        if match:\n",
    "            attrs = json.loads(match.group())\n",
    "            \n",
    "            # Convert same_day_exclusions array to string for Excel\n",
    "            if attrs.get('same_day_exclusions') and isinstance(attrs['same_day_exclusions'], list):\n",
    "                attrs['same_day_exclusions'] = ', '.join(attrs['same_day_exclusions'])\n",
    "            \n",
    "            phase2_results.append({\n",
    "                '_unique_key': unique_key,\n",
    "                **attrs\n",
    "            })\n",
    "            \n",
    "            n_filled = sum(1 for v in attrs.values() if v is not None and v != 'null' and v != '')\n",
    "            print(f\"  {code_info['Code']} ({code_info.get('Level_1_Section', '')[:20]}): {n_filled} attributes\")\n",
    "        else:\n",
    "            print(f\"  {code_info['Code']}: No JSON found\")\n",
    "            phase2_results.append({'_unique_key': unique_key})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  {code_info['Code']}: Error - {e}\")\n",
    "        phase2_results.append({'_unique_key': unique_key})\n",
    "\n",
    "print(f\"\\nPhase 2 complete: {len(phase2_results)} codes processed\")\n",
    "print(f\"Total API cost: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Combine Phase 1 + Phase 2 and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Phase 1 and Phase 2 results\n",
    "\n",
    "df_phase1 = pd.DataFrame(all_results)\n",
    "df_phase2 = pd.DataFrame(phase2_results)\n",
    "\n",
    "# Merge on _unique_key\n",
    "df_combined = df_phase1.merge(df_phase2, on='_unique_key', how='left')\n",
    "\n",
    "# Drop internal column\n",
    "df_combined = df_combined.drop(columns=['_unique_key'])\n",
    "\n",
    "# Reorder columns\n",
    "column_order = [\n",
    "    'AB_Code', 'AB_Description', 'AB_Fee', 'Target_Province',\n",
    "    'Code', 'Description', 'Fee', 'Type', 'Modality', 'Specialty',\n",
    "    'Links_To', 'Condition', 'Reasoning',\n",
    "    'Level_1_Section', 'Level_2_Subsection', 'Page_Found',\n",
    "    'modality', 'minimum_time_minutes', 'frequency_per_day', 'frequency_per_year',\n",
    "    'frequency_per_year_period', 'same_day_exclusions', 'premium_extended_hours',\n",
    "    'premium_location', 'premium_age', 'premium_other', 'additional_notes'\n",
    "]\n",
    "\n",
    "final_columns = [c for c in column_order if c in df_combined.columns]\n",
    "df_combined = df_combined[final_columns]\n",
    "\n",
    "print(f\"Combined DataFrame: {len(df_combined)} rows, {len(df_combined.columns)} columns\")\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Save Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_file = 'mb_section_pilot_complete.xlsx'\n",
    "df_combined.to_excel(output_file, index=False)\n",
    "print(f\"\\nSaved: {output_file}\")\n",
    "print(f\"  - Rows: {len(df_combined)}\")\n",
    "print(f\"  - Columns: {len(df_combined.columns)}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- SUMMARY ---\")\n",
    "print(f\"Total codes: {len(df_combined)}\")\n",
    "print(f\"  - PRIMARY: {len(df_combined[df_combined['Type'] == 'PRIMARY'])}\")\n",
    "print(f\"  - ADD-ON: {len(df_combined[df_combined['Type'] == 'ADD-ON'])}\")\n",
    "\n",
    "print(f\"\\n--- BY SECTION ---\")\n",
    "section_counts = df_combined.groupby('Level_1_Section').size().sort_index()\n",
    "for section, count in section_counts.items():\n",
    "    print(f\"  {section[:50]:50} | {count:3}\")\n",
    "\n",
    "print(f\"\\n--- COST ---\")\n",
    "print(f\"Total API calls: {total_calls}\")\n",
    "print(f\"Total cost: ${total_cost:.2f}\")\n",
    "\n",
    "# Download\n",
    "files.download(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
