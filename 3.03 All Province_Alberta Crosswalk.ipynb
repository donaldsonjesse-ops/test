{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.03 All Province Alberta Crosswalk\n",
    "\n",
    "**Consolidated notebook for mapping Alberta billing codes to BC, MB, ON, and SK equivalents.**\n",
    "\n",
    "## Workflow\n",
    "1. Upload all files (PDFs, Reference CSVs, Taxonomy)\n",
    "2. Configure Alberta code to match\n",
    "3. Run each province individually\n",
    "4. Combine results\n",
    "\n",
    "## Province-Specific Features\n",
    "| Province | Chunking | Special Features |\n",
    "|----------|----------|------------------|\n",
    "| BC | Level 1 | Code prefixes (P, G, PG) |\n",
    "| MB | Level 1 | Specialty-based fees |\n",
    "| ON | Level 2 | H/P settings, Surg/Asst/Anae fees |\n",
    "| SK | Level 1 | Referred/Not Referred dual-fees, Age premiums |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 1: Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas pdfplumber openpyxl tqdm PyMuPDF -q\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from google.colab import files\n",
    "\n",
    "print(\"All dependencies loaded.\")\n",
    "print(\"Ready to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 2: Upload Files\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2a: Upload Province PDFs\n",
    "\n",
    "Upload all 4 province schedule PDFs. Files will be auto-detected by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2a: Upload Province Schedule PDFs\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExpected files:\")\n",
    "print(\"  - BC Payment Schedule - March 31, 2024.pdf\")\n",
    "print(\"  - MB Payment Schedule - April 1, 2024.pdf\")\n",
    "print(\"  - ON - February 20, 2024 (effective April 1, 2024).pdf\")\n",
    "print(\"  - SK Payment Schedule - April 1, 2024.pdf\")\n",
    "print()\n",
    "\n",
    "uploaded_pdfs = files.upload()\n",
    "\n",
    "# Auto-detect province from filename\n",
    "PDF_FILES = {'BC': None, 'MB': None, 'ON': None, 'SK': None}\n",
    "\n",
    "for filename in uploaded_pdfs.keys():\n",
    "    filename_upper = filename.upper()\n",
    "    if 'BC' in filename_upper:\n",
    "        PDF_FILES['BC'] = filename\n",
    "    elif 'MB' in filename_upper:\n",
    "        PDF_FILES['MB'] = filename\n",
    "    elif 'ON' in filename_upper:\n",
    "        PDF_FILES['ON'] = filename\n",
    "    elif 'SK' in filename_upper:\n",
    "        PDF_FILES['SK'] = filename\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Detected PDFs:\")\n",
    "print(\"=\"*70)\n",
    "for prov, f in PDF_FILES.items():\n",
    "    status = \"✓\" if f else \"✗ MISSING\"\n",
    "    print(f\"  {prov}: {status} {f if f else ''}\")\n",
    "\n",
    "# Warn if any missing\n",
    "missing = [p for p, f in PDF_FILES.items() if f is None]\n",
    "if missing:\n",
    "    print(f\"\\n⚠️  WARNING: Missing PDFs for: {', '.join(missing)}\")\n",
    "    print(\"    You can still run the provinces that have PDFs.\")\n",
    "else:\n",
    "    print(\"\\n✓ All 4 province PDFs loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2b: Upload Section Reference CSVs\n",
    "\n",
    "Upload all 4 section reference CSVs. Files will be auto-detected by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STEP 2b: Upload Section Reference CSVs\")\nprint(\"=\"*70)\nprint(\"\\nExpected files:\")\nprint(\"  - bc_section_reference_simple.csv\")\nprint(\"  - manitoba_section_reference_final.csv\")\nprint(\"  - on_section_reference_full.csv\")\nprint(\"  - sk_section_reference_simple.csv\")\nprint()\n\nuploaded_refs = files.upload()\n\n# Auto-detect province from filename\nREF_FILES = {'BC': None, 'MB': None, 'ON': None, 'SK': None}\n\nfor filename in uploaded_refs.keys():\n    filename_lower = filename.lower()\n    # Check more specific patterns first, and use prefix patterns to avoid false matches\n    # (e.g., 'on' in 'section' was causing sk files to match ON)\n    if filename_lower.startswith('bc') or '_bc_' in filename_lower:\n        REF_FILES['BC'] = filename\n    elif 'mb' in filename_lower or 'manitoba' in filename_lower:\n        REF_FILES['MB'] = filename\n    elif filename_lower.startswith('sk') or '_sk_' in filename_lower:\n        REF_FILES['SK'] = filename\n    elif filename_lower.startswith('on') or '_on_' in filename_lower:\n        REF_FILES['ON'] = filename\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Detected Reference CSVs:\")\nprint(\"=\"*70)\nfor prov, f in REF_FILES.items():\n    status = \"✓\" if f else \"✗ MISSING\"\n    print(f\"  {prov}: {status} {f if f else ''}\")\n\n# Warn if any missing\nmissing = [p for p, f in REF_FILES.items() if f is None]\nif missing:\n    print(f\"\\n⚠️  WARNING: Missing reference CSVs for: {', '.join(missing)}\")\n    print(\"    You can still run the provinces that have reference files.\")\nelse:\n    print(\"\\n✓ All 4 province reference CSVs loaded successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2c: Upload Extraction Taxonomy\n",
    "\n",
    "Upload the extraction taxonomy Excel file for Phase 2 attribute extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2c: Upload Extraction Taxonomy\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExpected file:\")\n",
    "print(\"  - extraction_taxonomy.xlsx\")\n",
    "print()\n",
    "\n",
    "uploaded_tax = files.upload()\n",
    "\n",
    "TAXONOMY_FILE = list(uploaded_tax.keys())[0]\n",
    "df_taxonomy = pd.read_excel(TAXONOMY_FILE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Loaded Taxonomy: {TAXONOMY_FILE}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{len(df_taxonomy)} attributes:\")\n",
    "for _, row in df_taxonomy.iterrows():\n",
    "    print(f\"  - {row['attribute']}: {row['data_type']}\")\n",
    "\n",
    "# Build taxonomy reference string for prompts\n",
    "taxonomy_reference = \"\\n\".join([\n",
    "    f\"- {row['attribute']} ({row['data_type']}): {row['definition']} Taxonomy: {row['taxonomy']}\"\n",
    "    for _, row in df_taxonomy.iterrows()\n",
    "])\n",
    "\n",
    "print(\"\\n✓ Taxonomy loaded and ready for Phase 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: API Key\n",
    "\n",
    "Enter your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2d: API Key\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "OPENAI_API_KEY = \"\"  # <-- Paste your key here, or leave blank to use getpass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    from getpass import getpass\n",
    "    OPENAI_API_KEY = getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"\\n✓ API client initialized.\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE - Ready to configure Alberta code and run provinces\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 3: Alberta Code Configuration\n",
    "---\n",
    "\n",
    "**Edit this cell to change the Alberta code being mapped.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Alberta Code Config\n",
    "\n",
    "⚠️ **EDIT THIS CELL** to map a different Alberta code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ALBERTA CODE CONFIGURATION\n",
    "# ============================================================================\n",
    "# Edit this section to map a different Alberta billing code.\n",
    "# The province configs (Cell 5) should NOT need to change.\n",
    "# ============================================================================\n",
    "\n",
    "ALBERTA_CODE_CONFIG = {\n",
    "    # Basic code info\n",
    "    'code': '03.03CV',\n",
    "    'description': 'Telehealth consultation',\n",
    "    'fee': 25.09,\n",
    "    \n",
    "    # Clinical definition - describes the service in detail\n",
    "    'clinical_definition': \"\"\"Assessment of a patient's condition via telephone or secure videoconference.\n",
    "\n",
    "NOTE:\n",
    "- At minimum: limited assessment requiring history related to presenting problems, appropriate records review, and advice to the patient\n",
    "- Total physician time spent providing patient care must be MINIMUM 10 MINUTES\n",
    "- If less than 10 minutes same day, must use HSC 03.01AD instead\n",
    "- May only be claimed if service was initiated by the patient or their agent\n",
    "- May only be claimed if service is personally rendered by the physician\n",
    "- Benefit includes ordering appropriate diagnostic tests and discussion with patient\n",
    "- Patient record must include detailed summary of all services including start/stop times\n",
    "- Time spent on administrative tasks cannot be claimed\n",
    "- May NOT be claimed same day as: 03.01AD, 03.01S, 03.01T, 03.03FV, 03.05JR, 03.08CV, 08.19CV, 08.19CW, or 08.19CX by same physician for same patient\n",
    "- May NOT be claimed same day as in-person visit or consultation by same physician for same patient\n",
    "\n",
    "Category: V Visit (Virtual)\n",
    "Base rate: $25.09\"\"\",\n",
    "    \n",
    "    # Service type context - helps LLM understand what we're looking for\n",
    "    'service_context': \"\"\"This is a BASIC PATIENT-FACING virtual visit by any physician (not specialist-specific, not physician-to-physician).\"\"\",\n",
    "    \n",
    "    # What to search for (specific to this AB code type)\n",
    "    'search_criteria': \"\"\"\n",
    "WHAT TO LOOK FOR:\n",
    "- Virtual visits / virtual care\n",
    "- Telephone consultations / assessments\n",
    "- Video consultations / assessments\n",
    "- Telehealth codes\n",
    "- Any code that can be billed for a patient-facing virtual encounter\n",
    "\"\"\",\n",
    "    \n",
    "    # What to exclude (specific to this AB code type)\n",
    "    'exclusion_criteria': \"\"\"\n",
    "DO NOT INCLUDE:\n",
    "- Physician-to-physician consultations (e-consults between doctors)\n",
    "- E-assessments / e-consults (specialist-to-PCP) - not patient-facing\n",
    "- In-person only codes\n",
    "- Diagnostic procedures (ECG, imaging, labs)\n",
    "- Codes you cannot find literally in the text\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "# Display config\n",
    "print(\"=\"*70)\n",
    "print(\"ALBERTA CODE CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCode: {ALBERTA_CODE_CONFIG['code']}\")\n",
    "print(f\"Description: {ALBERTA_CODE_CONFIG['description']}\")\n",
    "print(f\"Fee: ${ALBERTA_CODE_CONFIG['fee']}\")\n",
    "print(\"\\n✓ Alberta code configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 4: Province Configurations\n",
    "---\n",
    "\n",
    "**DO NOT EDIT** unless province schedule structure changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Province Configs\n",
    "\n",
    "Static configurations for each province's schedule structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# PROVINCE CONFIGURATIONS\n# ============================================================================\n# Static configurations for each province's schedule structure.\n# These should NOT change when mapping different Alberta codes.\n# ============================================================================\n\nPROVINCE_CONFIGS = {\n    'BC': {\n        'name': 'British Columbia',\n        'chunking_level': 1,  # Level 1 sections\n        'rules_pages': (1, 52),  # General Preamble\n        'skip_sections': [\n            \"1. GENERAL PREAMBLE TO THE PAYMENT SCHEDULE\",\n            \"2. OUT-OF-OFFICE HOURS PREMIUMS\",\n        ],\n        'special_fields': [],  # No special fields\n        'extraction_rules': \"\"\"\nBC-SPECIFIC EXTRACTION RULES:\n\n1. **CODE PREFIXES** (indicate payment type, NOT setting):\n   - P = Professional fee\n   - G = Group fee\n   - PG = Professional + Group\n\n2. **FEE EXTRACTION**: Copy the exact fee value as shown\n\nACCURACY RULES - YOU MUST FOLLOW:\n\n1. **ONLY REAL CODES**: Return ONLY codes that LITERALLY appear in the text above.\n   - Copy the EXACT code as shown (e.g., 00100, 14051, 97017)\n   - If you cannot find the exact code string in the text, DO NOT include it\n   - NEVER invent, fabricate, or guess codes\n\n2. **EXACT VALUES**: Copy fee EXACTLY as shown in the document\n   - Use exact decimal values (e.g., \"25.43\" not \"25.00\")\n   - If fee is percentage-based premium, use \"-\" and explain in condition\n\n3. **FULL DESCRIPTIONS - CLIENT READY FORMAT**:\n   - Copy the COMPLETE service description as written in the schedule\n   - Do NOT abbreviate (write \"Telephone/video consultation\" not \"Tel consult\")\n   - Do NOT truncate (include the full description text)\n   - Use sentence case for consistency (capitalize first word and proper nouns)\n   - Include qualifying details (e.g., \"minimum 10 minutes\")\n   - Format: Clear, professional, ready for client delivery\n\n4. **MODALITY**: Only include modalities explicitly stated\n   - \"telephone\" = text says telephone/phone only\n   - \"video\" = text says video/videoconference only\n   - \"both\" = text explicitly allows BOTH, or doesn't restrict\n\n5. **PAGE NUMBERS**: page_found must match the \"=== PAGE X ===\" marker where code appears\n\n6. **SECTION HEADING**: Extract the subsection heading the code appears under\n   - Look for bold/uppercase headings\n   - This becomes level_2_subsection\n\"\"\",\n        'json_schema': {\n            'primary_codes': ['code', 'description', 'fee', 'modality', 'page_found', 'section_heading', 'reasoning'],\n            'add_on_codes': ['code', 'description', 'fee', 'modality', 'page_found', 'section_heading', 'links_to', 'condition']\n        },\n        'output_columns': [\n            'AB_Code', 'AB_Description', 'AB_Fee', 'Target_Province',\n            'Code', 'Description', 'Fee', 'Type', 'Modality', 'Specialty',\n            'Links_To', 'Condition', 'Reasoning',\n            'Level_1_Section', 'Level_2_Subsection', 'Page_Found'\n        ]\n    },\n\n    'MB': {\n        'name': 'Manitoba',\n        'chunking_level': 1,  # Level 1 sections\n        'rules_pages': (1, 82),  # Rules of Application\n        'skip_sections': [\n            \"APPENDICES\",\n        ],\n        'min_clinical_page': 83,  # Clinical sections start at page 83\n        'special_fields': [],  # No special fields\n        'extraction_rules': \"\"\"\nMB-SPECIFIC EXTRACTION RULES:\n\n1. **SPECIALTY-BASED FEES**: Each specialty section may have its own fee schedules\n\nACCURACY RULES - YOU MUST FOLLOW:\n\n1. **ONLY REAL CODES**: Return ONLY codes that LITERALLY appear in the text above.\n   - Copy the EXACT code as shown (e.g., 8321, 8340, 8447)\n   - If you cannot find the exact code string in the text, DO NOT include it\n   - NEVER invent, fabricate, or guess codes\n\n2. **EXACT VALUES**: Copy fee EXACTLY as shown in the document\n   - Use exact decimal values (e.g., \"59.05\" not \"59.00\")\n   - If fee is percentage-based premium, use \"-\" and explain in condition\n\n3. **FULL DESCRIPTIONS - CLIENT READY FORMAT**:\n   - Copy the COMPLETE service description as written in the schedule\n   - Do NOT abbreviate (write \"Virtual visit by telephone or video\" not \"Virtual visit\")\n   - Do NOT truncate (include the full description text)\n   - Use sentence case for consistency (capitalize first word and proper nouns)\n   - Include qualifying details (e.g., \"minimum 10 minutes\")\n   - Format: Clear, professional, ready for client delivery\n\n4. **MODALITY**: Only include modalities explicitly stated\n   - \"telephone\" = text says telephone/phone only\n   - \"video\" = text says video/videoconference only\n   - \"both\" = text explicitly allows BOTH, or doesn't restrict\n\n5. **PAGE NUMBERS**: page_found must match the \"=== PAGE X ===\" marker where code appears\n\n6. **SECTION HEADING**: Extract the subsection heading the code appears under\n   - Look for bold/uppercase headings like \"VIRTUAL VISITS\", \"HOSPITAL CARE\", etc.\n   - This becomes level_2_subsection\n\"\"\",\n        'json_schema': {\n            'primary_codes': ['code', 'description', 'fee', 'modality', 'page_found', 'section_heading', 'reasoning'],\n            'add_on_codes': ['code', 'description', 'fee', 'modality', 'page_found', 'section_heading', 'links_to', 'condition']\n        },\n        'output_columns': [\n            'AB_Code', 'AB_Description', 'AB_Fee', 'Target_Province',\n            'Code', 'Description', 'Fee', 'Type', 'Modality', 'Specialty',\n            'Links_To', 'Condition', 'Reasoning',\n            'Level_1_Section', 'Level_2_Subsection', 'Page_Found'\n        ]\n    },\n\n    'ON': {\n        'name': 'Ontario',\n        'chunking_level': 2,  # Level 2 sections (more granular)\n        'rules_pages': (1, 126),  # General Preamble\n        'skip_sections': [\n            \"General Preamble\",\n            \"Appendix A\",\n            \"Appendix B\",\n            \"Appendix C\",\n            \"Appendix D\",\n            \"Appendix F\",\n            \"Appendix G\",\n            \"Appendix H\",\n            \"Appendix J\",\n            \"Appendix Q\",\n            \"Numeric Index\",\n        ],\n        'special_fields': ['Fee_Type', 'Setting', 'Level_3_Heading'],\n        'extraction_rules': \"\"\"\nONTARIO-SPECIFIC EXTRACTION RULES:\n\n1. **H/P COLUMNS (Setting)**:\n   - If a code has BOTH H (Hospital) and P (Professional/Office) fees, create SEPARATE entries for each\n   - H = Hospital setting, P = Professional/Office setting\n   - If only one fee exists, use that setting\n\n2. **SURGICAL FEE COLUMNS**:\n   - Surg = Surgeon fee -> create entry with fee_type \"Surgeon\"\n   - Asst = Assistant fee -> create entry with fee_type \"Assistant\" (skip if \"nil\")\n   - Anae = Anaesthesia units -> create entry with fee_type \"Anaesthesia\" (these are TIME UNITS, not dollars)\n\n3. **CODE PREFIXES** (indicate service type, NOT setting):\n   - A = Assessments/consultations\n   - E = Diagnostic/therapeutic procedures\n   - G = General listings\n   - K = Special visit premiums\n   - Z = Surgical procedures\n\nACCURACY RULES:\n\n1. **ONLY REAL CODES**: Return ONLY codes that LITERALLY appear in the text above.\n   - Copy the EXACT code as shown (e.g., A003, K017, Z101)\n   - NEVER invent, fabricate, or guess codes\n\n2. **EXACT VALUES**: Copy fee EXACTLY as shown in the document\n   - Use exact decimal values (e.g., \"87.35\" not \"87.00\")\n   - For Anae column, these are UNITS not dollars\n\n3. **FULL DESCRIPTIONS - CLIENT READY FORMAT**:\n   - Copy the COMPLETE service description as written in the schedule\n   - Do NOT abbreviate or truncate\n   - Use sentence case for consistency\n   - Include qualifying details (e.g., \"minimum 50 minutes\")\n\n4. **LEVEL 3 EXTRACTION**:\n   - Extract the subsection heading the code appears under (e.g., \"INCISION\", \"EXCISION\", \"GENERAL LISTINGS\")\n   - This becomes level_3_heading\n\n5. **MODALITY**: Only include modalities explicitly stated\n   - \"telephone\" = text says telephone/phone\n   - \"video\" = text says video/videoconference\n   - \"both\" = text explicitly allows BOTH, or doesn't restrict\n\nIMPORTANT: For codes with multiple fee types (Surg/Asst/Anae) or settings (H/P), create SEPARATE entries for each combination.\n\"\"\",\n        'json_schema': {\n            'codes': ['code', 'description', 'fee', 'fee_type', 'setting', 'modality', 'page_found',\n                     'level_3_heading', 'is_addon', 'links_to', 'condition', 'reasoning']\n        },\n        'output_columns': [\n            'AB_Code', 'AB_Description', 'AB_Fee', 'Target_Province',\n            'Code', 'Description', 'Fee', 'Fee_Type', 'Setting', 'Type', 'Modality',\n            'Links_To', 'Condition', 'Reasoning',\n            'Level_1_Section', 'Level_2_Subsection', 'Level_3_Heading', 'Page_Found'\n        ]\n    },\n\n    'SK': {\n        'name': 'Saskatchewan',\n        'chunking_level': 1,  # Level 1 sections\n        'rules_pages': (1, 70),  # Preamble/Rules\n        'skip_sections': [\n            \"Introduction\",\n            \"To Request a Change to the Payment Schedule\",\n            \"Services Provided Outside Saskatchewan\",\n            \"Billing For Services Provided To Out-Of-Province Beneficiaries\",\n            \"Definitions\",\n            \"Documentation Requirements\",\n            \"Services Billable by Entitlement or by Approval\",\n            \"Assessment Rules\",\n            \"General Information\",\n            \"Services Not Insured by the Ministry of Health\",\n            \"Assessment of Accounts\",\n            \"Verification Program\",\n            \"Information Sources\",\n            \"Reciprocal Billing\",\n            \"Explanatory Codes for Physicians\",\n        ],\n        'min_clinical_page': 71,  # Clinical sections start at page 71\n        'special_fields': ['Fee_Type', 'Age_Premium_Applies'],\n        'extraction_rules': \"\"\"\nSASKATCHEWAN-SPECIFIC EXTRACTION RULES:\n\n1. **DUAL-FEE STRUCTURE (Referred vs Not Referred)**:\n   - Many SK codes have TWO fees: \"Referred\" and \"Not Referred\"\n   - If a code has BOTH fees, create SEPARATE entries for each:\n     - One entry with fee_type=\"Referred\" and the referred fee\n     - One entry with fee_type=\"Not Referred\" and the not-referred fee\n   - If only one fee exists, use fee_type=\"Standard\"\n\n2. **AGE PREMIUMS (Section-Wide)**:\n   - SK has age-based premiums for patients 0-5 years and 65+ years\n   - If the section header or preamble states age premiums apply to ALL codes in the section, note this for EVERY code\n   - Do NOT skip age premiums just because they're not repeated per-code\n\nACCURACY RULES - YOU MUST FOLLOW:\n\n1. **ONLY REAL CODES**: Return ONLY codes that LITERALLY appear in the text above.\n   - Copy the EXACT code as shown\n   - If you cannot find the exact code string in the text, DO NOT include it\n   - NEVER invent, fabricate, or guess codes\n\n2. **EXACT VALUES**: Copy fee EXACTLY as shown in the document\n   - Use exact decimal values (e.g., \"45.50\" not \"45.00\")\n   - If fee is percentage-based premium, use \"-\" and explain in condition\n\n3. **FULL DESCRIPTIONS - CLIENT READY FORMAT**:\n   - Copy the COMPLETE service description as written in the schedule\n   - Do NOT abbreviate (write \"Telephone/video consultation\" not \"Tel consult\")\n   - Do NOT truncate (include the full description text)\n   - Use sentence case for consistency (capitalize first word and proper nouns)\n   - Include qualifying details (e.g., \"minimum 10 minutes\")\n   - Format: Clear, professional, ready for client delivery\n\n4. **MODALITY**: Only include modalities explicitly stated\n   - \"telephone\" = text says telephone/phone only\n   - \"video\" = text says video/videoconference only\n   - \"both\" = text explicitly allows BOTH, or doesn't restrict\n\n5. **PAGE NUMBERS**: page_found must match the \"=== PAGE X ===\" marker where code appears\n\n6. **SECTION HEADING**: Extract the subsection heading the code appears under\n   - Look for bold/uppercase headings\n   - This becomes level_2_subsection\n\nIMPORTANT: For codes with BOTH Referred and Not Referred fees, create SEPARATE entries for each fee type.\n\"\"\",\n        'json_schema': {\n            'primary_codes': ['code', 'description', 'fee', 'fee_type', 'modality', 'page_found',\n                             'section_heading', 'age_premium_applies', 'reasoning'],\n            'add_on_codes': ['code', 'description', 'fee', 'fee_type', 'modality', 'page_found',\n                            'section_heading', 'age_premium_applies', 'links_to', 'condition']\n        },\n        'output_columns': [\n            'AB_Code', 'AB_Description', 'AB_Fee', 'Target_Province',\n            'Code', 'Description', 'Fee', 'Fee_Type', 'Type', 'Modality', 'Specialty',\n            'Links_To', 'Condition', 'Reasoning',\n            'Level_1_Section', 'Level_2_Subsection', 'Page_Found', 'Age_Premium_Applies'\n        ]\n    }\n}\n\n# Display loaded configs\nprint(\"=\"*70)\nprint(\"PROVINCE CONFIGURATIONS LOADED\")\nprint(\"=\"*70)\nfor prov, config in PROVINCE_CONFIGS.items():\n    print(f\"\\n{prov} ({config['name']}):\")\n    print(f\"  - Chunking: Level {config['chunking_level']}\")\n    print(f\"  - Rules pages: {config['rules_pages'][0]}-{config['rules_pages'][1]}\")\n    print(f\"  - Skip sections: {len(config['skip_sections'])}\")\n    print(f\"  - Special fields: {config['special_fields'] or 'None'}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"All province configurations loaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 5: Shared Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Shared Functions\n",
    "\n",
    "Core processing functions used by all provinces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SHARED FUNCTIONS\n# ============================================================================\n# Core processing functions used by all provinces.\n# These work with ALBERTA_CODE_CONFIG and PROVINCE_CONFIGS defined above.\n# ============================================================================\n\n# --- Cost Tracking ---\ntotal_cost = 0.0\ntotal_calls = 0\n\ndef reset_cost_tracking():\n    \"\"\"Reset cost tracking for a new province run.\"\"\"\n    global total_cost, total_calls\n    total_cost = 0.0\n    total_calls = 0\n\ndef track_cost(inp_tokens, out_tokens):\n    \"\"\"Track API costs (GPT-4 pricing estimate).\"\"\"\n    global total_cost, total_calls\n    total_cost += (inp_tokens/1e6)*3.0 + (out_tokens/1e6)*15.0\n    total_calls += 1\n\n# --- Dynamic Token Limits ---\ndef get_dynamic_max_tokens(char_count):\n    \"\"\"Set max_completion_tokens based on section size for Phase 1.\"\"\"\n    if char_count > 150000:\n        return 20000\n    elif char_count > 80000:\n        return 14000\n    elif char_count > 40000:\n        return 10000\n    elif char_count > 15000:\n        return 6000\n    else:\n        return 4000\n\ndef get_phase2_max_tokens(rules_char_count):\n    \"\"\"Set max_completion_tokens for Phase 2 based on rules size.\n    \n    Larger rules = more context for LLM to process = need more output buffer.\n    \"\"\"\n    if rules_char_count > 300000:\n        return 4000\n    elif rules_char_count > 200000:\n        return 3000\n    else:\n        return 2500\n\n# --- PDF Loading ---\ndef load_pdf_pages(pdf_path):\n    \"\"\"Load all pages from PDF into a dictionary.\"\"\"\n    pdf_pages = {}\n    with pdfplumber.open(pdf_path) as pdf:\n        total_pages = len(pdf.pages)\n        for i, page in enumerate(tqdm(pdf.pages, desc=\"Loading pages\")):\n            page_num = i + 1\n            try:\n                text = page.extract_text()\n                if text:\n                    pdf_pages[page_num] = text\n            except:\n                pass\n    return pdf_pages, total_pages\n\n# --- Section Chunking ---\ndef build_section_chunks_level1(df_ref, pdf_pages, total_pages, prov_config):\n    \"\"\"Build section chunks for Level 1 chunking (BC, MB, SK).\"\"\"\n    skip_sections = prov_config['skip_sections']\n    min_page = prov_config.get('min_clinical_page', 1)\n    \n    # Get unique Level 1 sections with their minimum page_start\n    level_1_sections = df_ref.groupby('level_1')['page_start'].min().sort_values()\n    level_1_list = list(level_1_sections.items())\n    \n    section_chunks = {}\n    \n    for idx, (section_name, start_page) in enumerate(level_1_list):\n        # Skip configured sections\n        if section_name in skip_sections:\n            continue\n        \n        # Skip pages before clinical content\n        if start_page < min_page:\n            continue\n        \n        # End page is start of next section - 1, or last page\n        if idx + 1 < len(level_1_list):\n            end_page = level_1_list[idx + 1][1] - 1\n        else:\n            end_page = total_pages\n        \n        # Extract text\n        section_text = \"\"\n        pages_in_section = []\n        for pg in range(start_page, end_page + 1):\n            if pg in pdf_pages:\n                section_text += f\"\\n=== PAGE {pg} ===\\n{pdf_pages[pg]}\"\n                pages_in_section.append(pg)\n        \n        section_chunks[section_name] = {\n            'text': section_text,\n            'level_1': section_name,\n            'level_2': section_name,\n            'start_page': start_page,\n            'end_page': end_page,\n            'page_count': len(pages_in_section),\n            'char_count': len(section_text)\n        }\n    \n    return section_chunks\n\ndef build_section_chunks_level2(df_ref, pdf_pages, total_pages, prov_config):\n    \"\"\"Build section chunks for Level 2 chunking (ON).\"\"\"\n    skip_sections = prov_config['skip_sections']\n    \n    # Fill empty level_2 with level_1\n    df_ref = df_ref.copy()\n    df_ref['level_2'] = df_ref['level_2'].fillna('')\n    \n    section_chunks = {}\n    \n    for idx, row in df_ref.iterrows():\n        level_1 = row['level_1']\n        level_2 = row['level_2'] if row['level_2'] else level_1\n        start_page = int(row['page_start'])\n        \n        # Skip configured sections\n        if level_1 in skip_sections:\n            continue\n        \n        # Section key\n        section_key = f\"{level_1} | {level_2}\" if level_2 != level_1 else level_1\n        \n        # End page\n        if idx + 1 < len(df_ref):\n            end_page = int(df_ref.iloc[idx + 1]['page_start']) - 1\n        else:\n            end_page = total_pages\n        \n        # Extract text\n        section_text = \"\"\n        pages_in_section = []\n        for pg in range(start_page, end_page + 1):\n            if pg in pdf_pages:\n                section_text += f\"\\n=== PAGE {pg} ===\\n{pdf_pages[pg]}\"\n                pages_in_section.append(pg)\n        \n        section_chunks[section_key] = {\n            'text': section_text,\n            'level_1': level_1,\n            'level_2': level_2,\n            'start_page': start_page,\n            'end_page': end_page,\n            'page_count': len(pages_in_section),\n            'char_count': len(section_text)\n        }\n    \n    return section_chunks\n\n# --- Rules Extraction ---\ndef extract_rules_text(pdf_path, rules_pages):\n    \"\"\"Extract rules/preamble text from PDF.\"\"\"\n    start_page, end_page = rules_pages\n    rules_text = \"\"\n    \n    src_pdf = fitz.open(pdf_path)\n    for page_num in range(start_page - 1, end_page):\n        page = src_pdf[page_num]\n        text = page.get_text()\n        if text:\n            rules_text += f\"\\n=== RULES PAGE {page_num + 1} ===\\n{text}\"\n    src_pdf.close()\n    \n    return rules_text\n\n# --- Phase 1 Prompt Builder ---\ndef build_phase1_prompt(section_key, section_info, prov_code, prov_config):\n    \"\"\"Build Phase 1 extraction prompt for a section.\"\"\"\n    ab = ALBERTA_CODE_CONFIG\n    section_text = section_info['text']\n    start_page = section_info['start_page']\n    end_page = section_info['end_page']\n    level_1 = section_info.get('level_1', section_key)\n    level_2 = section_info.get('level_2', section_key)\n    \n    # Province-specific JSON schema\n    if prov_code == 'ON':\n        json_template = '''{\n  \"section_key\": \"''' + section_key + '''\",\n  \"found\": true/false,\n  \"codes\": [\n    {\n      \"code\": \"EXACT code\",\n      \"description\": \"COMPLETE description\",\n      \"fee\": \"EXACT fee\",\n      \"fee_type\": \"Standard|Surgeon|Assistant|Anaesthesia\",\n      \"setting\": \"Hospital|Professional|N/A\",\n      \"modality\": \"telephone|video|both\",\n      \"page_found\": <integer>,\n      \"level_3_heading\": \"subsection heading\",\n      \"is_addon\": true/false,\n      \"links_to\": [],\n      \"condition\": \"\",\n      \"reasoning\": \"why this matches\"\n    }\n  ]\n}'''\n        no_match = '{\"section_key\": \"' + section_key + '\", \"found\": false, \"codes\": []}'\n    elif prov_code == 'SK':\n        json_template = '''{\n  \"section_name\": \"''' + section_key + '''\",\n  \"found\": true/false,\n  \"primary_codes\": [\n    {\n      \"code\": \"EXACT code\",\n      \"description\": \"COMPLETE description\",\n      \"fee\": \"EXACT fee\",\n      \"fee_type\": \"Referred|Not Referred|Standard\",\n      \"modality\": \"telephone|video|both\",\n      \"page_found\": <integer>,\n      \"section_heading\": \"subsection heading\",\n      \"age_premium_applies\": true/false,\n      \"reasoning\": \"why this matches\"\n    }\n  ],\n  \"add_on_codes\": [\n    {\n      \"code\": \"EXACT code\",\n      \"description\": \"COMPLETE description\",\n      \"fee\": \"EXACT fee\",\n      \"fee_type\": \"Referred|Not Referred|Standard\",\n      \"modality\": \"telephone|video|both\",\n      \"page_found\": <integer>,\n      \"section_heading\": \"subsection heading\",\n      \"age_premium_applies\": true/false,\n      \"links_to\": [],\n      \"condition\": \"\"\n    }\n  ]\n}'''\n        no_match = '{\"section_name\": \"' + section_key + '\", \"found\": false, \"primary_codes\": [], \"add_on_codes\": []}'\n    else:  # BC, MB\n        json_template = '''{\n  \"section_name\": \"''' + section_key + '''\",\n  \"found\": true/false,\n  \"primary_codes\": [\n    {\n      \"code\": \"EXACT code\",\n      \"description\": \"COMPLETE description\",\n      \"fee\": \"EXACT fee\",\n      \"modality\": \"telephone|video|both\",\n      \"page_found\": <integer>,\n      \"section_heading\": \"subsection heading\",\n      \"reasoning\": \"why this matches\"\n    }\n  ],\n  \"add_on_codes\": [\n    {\n      \"code\": \"EXACT code\",\n      \"description\": \"COMPLETE description\",\n      \"fee\": \"EXACT fee\",\n      \"modality\": \"telephone|video|both\",\n      \"page_found\": <integer>,\n      \"section_heading\": \"subsection heading\",\n      \"links_to\": [],\n      \"condition\": \"\"\n    }\n  ]\n}'''\n        no_match = '{\"section_name\": \"' + section_key + '\", \"found\": false, \"primary_codes\": [], \"add_on_codes\": []}'\n    \n    return f\"\"\"You are a senior physician billing specialist mapping Alberta fee codes to {prov_config['name']} equivalents.\n\nALBERTA CODE TO MATCH:\n- Code: {ab['code']}\n- Description: {ab['description']}\n- Fee: ${ab['fee']}\n\nCLINICAL SERVICE DEFINITION:\n{ab['clinical_definition']}\n\n{ab['service_context']}\n\nYou are reviewing the section: {level_1} > {level_2}\nPages {start_page} to {end_page}\n\n{prov_config['name'].upper()} PAYMENT SCHEDULE - SECTION:\n\n{section_text}\n\nTASK:\nFind ALL {prov_config['name']} codes in this section that bill for patient-facing virtual assessments (telephone or video consultations with patients).\n\n{prov_config['extraction_rules']}\n\n{ab['search_criteria']}\n\n{ab['exclusion_criteria']}\n\nJSON only:\n{json_template}\n\nIf no telehealth/virtual codes in this section: {no_match}\"\"\"\n\n# --- Phase 2 Prompt Builder ---\ndef build_phase2_prompt(code_info, chunk_text, rules_text, prov_code):\n    \"\"\"Build Phase 2 attribute extraction prompt.\n    \n    IMPORTANT:\n    - rules_text: FULL (no truncation) - LLM needs all billing rules\n    - chunk_text: Truncated to 30K chars - just the code-specific section\n    \"\"\"\n    \n    # SK-specific additional rules checklist\n    sk_rules = \"\"\n    if prov_code == 'SK':\n        sk_rules = \"\"\"\nSASKATCHEWAN STANDARD RULES CHECKLIST:\n- 3,000 services per physician per year limit\n- Age premiums: 0-5 years and 65+ years eligible for premium\n- Verify if this code/section is eligible for age premiums\n\"\"\"\n    \n    return f\"\"\"You are a senior physician billing specialist extracting detailed attributes for a {prov_code} billing code.\n\nCODE TO ANALYZE:\n- Code: {code_info['Code']}\n- Description: {code_info['Description']}\n- Fee: {code_info['Fee']}\n- Type: {code_info['Type']}\n- Section: {code_info.get('Level_1_Section', 'N/A')}\n- Condition (from Phase 1): {code_info.get('Condition', 'N/A')}\n\nATTRIBUTES TO EXTRACT:\n{taxonomy_reference}\n{sk_rules}\nRULES/PREAMBLE (FULL - read carefully for billing rules applicable to this code):\n{rules_text}\n\nCODE-SPECIFIC SECTION:\n{chunk_text[:30000]}\n\nTASK:\nUsing ALL available information above, extract values for each attribute.\n\nINSTRUCTIONS:\n1. Read through the ENTIRE Rules/Preamble to find billing rules that apply to this code\n2. Look for: frequency limits, time requirements, same-day exclusions, premiums, conditions\n3. For each attribute, extract the value if found, or null if not explicitly stated\n4. For same_day_exclusions: return as array of code strings\n5. For additional_notes: ONLY include important billing information not captured elsewhere\n\nReturn JSON only:\n{{\n  \"modality\": \"telephone|video|both|in_person|asynchronous|null\",\n  \"minimum_time_minutes\": integer or null,\n  \"frequency_per_day\": integer or null,\n  \"frequency_per_year\": integer or null,\n  \"frequency_per_year_period\": \"annual|quarterly|90_days|monthly|null\",\n  \"same_day_exclusions\": [\"code1\", \"code2\"] or [] or null,\n  \"premium_extended_hours\": \"rate% code conditions\" or null,\n  \"premium_location\": \"rate% code conditions\" or null,\n  \"premium_age\": \"rate% conditions\" or null,\n  \"premium_other\": \"rate% code conditions\" or null,\n  \"additional_notes\": \"other important billing info\" or null\n}}\"\"\"\n\n# --- Result Processing ---\ndef process_phase1_result(result, section_key, section_info, prov_code, code_chunks):\n    \"\"\"Process Phase 1 JSON result into standardized rows.\"\"\"\n    ab = ALBERTA_CODE_CONFIG\n    rows = []\n    \n    if prov_code == 'ON':\n        # Ontario uses 'codes' array with fee_type and setting\n        for c in result.get('codes', []):\n            code = c.get('code', '')\n            fee = str(c.get('fee', ''))\n            fee_type = c.get('fee_type', 'Standard')\n            setting = c.get('setting', 'N/A')\n            modality = c.get('modality', '')\n            \n            unique_key = f\"{code}_{fee}_{fee_type}_{setting}_{section_key}\"\n            code_chunks[unique_key] = section_info['text']\n            \n            rows.append({\n                'AB_Code': ab['code'],\n                'AB_Description': ab['description'],\n                'AB_Fee': ab['fee'],\n                'Target_Province': prov_code,\n                'Code': code,\n                'Description': c.get('description', ''),\n                'Fee': c.get('fee', ''),\n                'Fee_Type': fee_type,\n                'Setting': setting,\n                'Type': 'ADD-ON' if c.get('is_addon') else 'PRIMARY',\n                'Modality': modality,\n                'Links_To': ', '.join(c.get('links_to', [])) if c.get('links_to') else '',\n                'Condition': c.get('condition', ''),\n                'Reasoning': c.get('reasoning', ''),\n                'Level_1_Section': section_info.get('level_1', section_key),\n                'Level_2_Subsection': section_info.get('level_2', ''),\n                'Level_3_Heading': c.get('level_3_heading', ''),\n                'Page_Found': c.get('page_found', ''),\n                '_unique_key': unique_key\n            })\n    \n    elif prov_code == 'SK':\n        # Saskatchewan uses primary_codes/add_on_codes with fee_type and age_premium\n        for p in result.get('primary_codes', []):\n            code = p.get('code', '')\n            fee = str(p.get('fee', ''))\n            fee_type = p.get('fee_type', 'Standard')\n            modality = p.get('modality', '')\n            \n            unique_key = f\"{code}_{fee}_{fee_type}_{modality}_{section_key}\"\n            code_chunks[unique_key] = section_info['text']\n            \n            rows.append({\n                'AB_Code': ab['code'],\n                'AB_Description': ab['description'],\n                'AB_Fee': ab['fee'],\n                'Target_Province': prov_code,\n                'Code': code,\n                'Description': p.get('description', ''),\n                'Fee': p.get('fee', ''),\n                'Fee_Type': fee_type,\n                'Type': 'PRIMARY',\n                'Modality': modality,\n                'Specialty': section_key,\n                'Links_To': '',\n                'Condition': '',\n                'Reasoning': p.get('reasoning', ''),\n                'Level_1_Section': section_key,\n                'Level_2_Subsection': p.get('section_heading', ''),\n                'Page_Found': p.get('page_found', ''),\n                'Age_Premium_Applies': p.get('age_premium_applies', False),\n                '_unique_key': unique_key\n            })\n        \n        for a in result.get('add_on_codes', []):\n            code = a.get('code', '')\n            fee = str(a.get('fee', ''))\n            fee_type = a.get('fee_type', 'Standard')\n            modality = a.get('modality', '')\n            \n            unique_key = f\"{code}_{fee}_{fee_type}_{modality}_{section_key}\"\n            code_chunks[unique_key] = section_info['text']\n            \n            rows.append({\n                'AB_Code': ab['code'],\n                'AB_Description': ab['description'],\n                'AB_Fee': ab['fee'],\n                'Target_Province': prov_code,\n                'Code': code,\n                'Description': a.get('description', ''),\n                'Fee': a.get('fee', ''),\n                'Fee_Type': fee_type,\n                'Type': 'ADD-ON',\n                'Modality': modality,\n                'Specialty': section_key,\n                'Links_To': ', '.join(a.get('links_to', [])) if a.get('links_to') else '',\n                'Condition': a.get('condition', ''),\n                'Reasoning': '',\n                'Level_1_Section': section_key,\n                'Level_2_Subsection': a.get('section_heading', ''),\n                'Page_Found': a.get('page_found', ''),\n                'Age_Premium_Applies': a.get('age_premium_applies', False),\n                '_unique_key': unique_key\n            })\n    \n    else:  # BC, MB\n        for p in result.get('primary_codes', []):\n            code = p.get('code', '')\n            fee = str(p.get('fee', ''))\n            modality = p.get('modality', '')\n            \n            unique_key = f\"{code}_{fee}_{modality}_{section_key}\"\n            code_chunks[unique_key] = section_info['text']\n            \n            rows.append({\n                'AB_Code': ab['code'],\n                'AB_Description': ab['description'],\n                'AB_Fee': ab['fee'],\n                'Target_Province': prov_code,\n                'Code': code,\n                'Description': p.get('description', ''),\n                'Fee': p.get('fee', ''),\n                'Type': 'PRIMARY',\n                'Modality': modality,\n                'Specialty': section_key,\n                'Links_To': '',\n                'Condition': '',\n                'Reasoning': p.get('reasoning', ''),\n                'Level_1_Section': section_key,\n                'Level_2_Subsection': p.get('section_heading', ''),\n                'Page_Found': p.get('page_found', ''),\n                '_unique_key': unique_key\n            })\n        \n        for a in result.get('add_on_codes', []):\n            code = a.get('code', '')\n            fee = str(a.get('fee', ''))\n            modality = a.get('modality', '')\n            \n            unique_key = f\"{code}_{fee}_{modality}_{section_key}\"\n            code_chunks[unique_key] = section_info['text']\n            \n            rows.append({\n                'AB_Code': ab['code'],\n                'AB_Description': ab['description'],\n                'AB_Fee': ab['fee'],\n                'Target_Province': prov_code,\n                'Code': code,\n                'Description': a.get('description', ''),\n                'Fee': a.get('fee', ''),\n                'Type': 'ADD-ON',\n                'Modality': modality,\n                'Specialty': section_key,\n                'Links_To': ', '.join(a.get('links_to', [])) if a.get('links_to') else '',\n                'Condition': a.get('condition', ''),\n                'Reasoning': '',\n                'Level_1_Section': section_key,\n                'Level_2_Subsection': a.get('section_heading', ''),\n                'Page_Found': a.get('page_found', ''),\n                '_unique_key': unique_key\n            })\n    \n    return rows\n\nprint(\"=\"*70)\nprint(\"SHARED FUNCTIONS LOADED\")\nprint(\"=\"*70)\nprint(\"\\nAvailable functions:\")\nprint(\"  - reset_cost_tracking()\")\nprint(\"  - track_cost(inp, out)\")\nprint(\"  - get_dynamic_max_tokens(char_count) - Phase 1\")\nprint(\"  - get_phase2_max_tokens(rules_char_count) - Phase 2\")\nprint(\"  - load_pdf_pages(pdf_path)\")\nprint(\"  - build_section_chunks_level1(df_ref, pdf_pages, total_pages, prov_config)\")\nprint(\"  - build_section_chunks_level2(df_ref, pdf_pages, total_pages, prov_config)\")\nprint(\"  - extract_rules_text(pdf_path, rules_pages)\")\nprint(\"  - build_phase1_prompt(section_key, section_info, prov_code, prov_config)\")\nprint(\"  - build_phase2_prompt(code_info, chunk_text, rules_text, prov_code)\")\nprint(\"  - process_phase1_result(result, section_key, section_info, prov_code, code_chunks)\")\nprint(\"\\nPhase 2: Full rules_text (no truncation), chunk_text[:30000]\")\nprint(\"Ready to process provinces.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 6: Run Provinces\n",
    "---\n",
    "\n",
    "Run each province individually. Results are saved and downloaded after each province completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7a: Run British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# BRITISH COLUMBIA - FULL CROSSWALK\n# ============================================================================\n\nPROV_CODE = 'BC'\nprov_config = PROVINCE_CONFIGS[PROV_CODE]\n\nprint(\"=\"*70)\nprint(f\"BRITISH COLUMBIA CROSSWALK\")\nprint(\"=\"*70)\n\n# Check files exist\nif not PDF_FILES.get(PROV_CODE) or not REF_FILES.get(PROV_CODE):\n    print(f\"ERROR: Missing files for {PROV_CODE}\")\n    print(f\"  PDF: {PDF_FILES.get(PROV_CODE)}\")\n    print(f\"  REF: {REF_FILES.get(PROV_CODE)}\")\nelse:\n    reset_cost_tracking()\n    \n    # Load reference CSV\n    df_ref = pd.read_csv(REF_FILES[PROV_CODE])\n    df_ref = df_ref.sort_values('page_start').reset_index(drop=True)\n    print(f\"Loaded {len(df_ref)} section entries from reference CSV\")\n    \n    # Load PDF pages\n    print(f\"\\nLoading PDF: {PDF_FILES[PROV_CODE]}\")\n    pdf_pages, total_pages = load_pdf_pages(PDF_FILES[PROV_CODE])\n    print(f\"Loaded {len(pdf_pages)} pages\")\n    \n    # Build section chunks (Level 1)\n    section_chunks = build_section_chunks_level1(df_ref, pdf_pages, total_pages, prov_config)\n    print(f\"\\nCreated {len(section_chunks)} section chunks\")\n    \n    # --- PHASE 1: Extract codes ---\n    print(f\"\\n{'='*70}\")\n    print(\"PHASE 1: EXTRACTING CODES\")\n    print(\"=\"*70)\n    \n    all_results_bc = []\n    code_chunks_bc = {}\n    \n    for section_key, section_info in tqdm(section_chunks.items(), desc=\"Processing sections\"):\n        char_count = section_info['char_count']\n        max_tokens = get_dynamic_max_tokens(char_count)\n        \n        prompt = build_phase1_prompt(section_key, section_info, PROV_CODE, prov_config)\n        \n        try:\n            resp = client.chat.completions.create(\n                model=\"gpt-5.1-2025-11-13\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_completion_tokens=max_tokens\n            )\n            track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n            \n            content = resp.choices[0].message.content\n            match = re.search(r'\\{[\\s\\S]*\\}', content)\n            \n            if match:\n                result = json.loads(match.group())\n                if result.get('found'):\n                    rows = process_phase1_result(result, section_key, section_info, PROV_CODE, code_chunks_bc)\n                    all_results_bc.extend(rows)\n                    print(f\"  [{section_key[:40]}] -> {len(rows)} codes\")\n        except Exception as e:\n            print(f\"  [{section_key[:40]}] ERROR: {e}\")\n    \n    print(f\"\\nPhase 1 complete: {len(all_results_bc)} codes found\")\n    \n    # --- PHASE 2: Extract attributes ---\n    if len(all_results_bc) > 0:\n        print(f\"\\n{'='*70}\")\n        print(\"PHASE 2: EXTRACTING ATTRIBUTES\")\n        print(\"=\"*70)\n        \n        rules_text = extract_rules_text(PDF_FILES[PROV_CODE], prov_config['rules_pages'])\n        print(f\"Loaded rules text: {len(rules_text):,} chars\")\n        \n        # Dynamic token limit based on rules size\n        phase2_tokens = get_phase2_max_tokens(len(rules_text))\n        print(f\"Phase 2 max_completion_tokens: {phase2_tokens}\")\n        \n        phase2_results = []\n        for code_info in tqdm(all_results_bc, desc=\"Extracting attributes\"):\n            unique_key = code_info.get('_unique_key', '')\n            chunk_text = code_chunks_bc.get(unique_key, '')\n            \n            prompt = build_phase2_prompt(code_info, chunk_text, rules_text, PROV_CODE)\n            \n            try:\n                resp = client.chat.completions.create(\n                    model=\"gpt-5.1-2025-11-13\",\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    temperature=0.1,\n                    max_completion_tokens=phase2_tokens\n                )\n                track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n                \n                content = resp.choices[0].message.content\n                match = re.search(r'\\{[\\s\\S]*\\}', content)\n                \n                if match:\n                    attrs = json.loads(match.group())\n                    if attrs.get('same_day_exclusions') and isinstance(attrs['same_day_exclusions'], list):\n                        attrs['same_day_exclusions'] = ', '.join(attrs['same_day_exclusions'])\n                    phase2_results.append({'_unique_key': unique_key, **attrs})\n                else:\n                    phase2_results.append({'_unique_key': unique_key})\n            except Exception as e:\n                phase2_results.append({'_unique_key': unique_key})\n        \n        # Combine Phase 1 + Phase 2\n        df_phase1 = pd.DataFrame(all_results_bc)\n        df_phase2 = pd.DataFrame(phase2_results)\n        df_bc = df_phase1.merge(df_phase2, on='_unique_key', how='left')\n        df_bc = df_bc.drop(columns=['_unique_key'])\n        \n        # Save\n        output_file = f\"3.02_BC_Alberta_Complete.xlsx\"\n        df_bc.to_excel(output_file, index=False)\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"BC COMPLETE\")\n        print(\"=\"*70)\n        print(f\"Total codes: {len(df_bc)}\")\n        print(f\"  - PRIMARY: {len(df_bc[df_bc['Type'] == 'PRIMARY'])}\")\n        print(f\"  - ADD-ON: {len(df_bc[df_bc['Type'] == 'ADD-ON'])}\")\n        print(f\"API calls: {total_calls} | Cost: ${total_cost:.2f}\")\n        print(f\"\\nSaved: {output_file}\")\n        \n        files.download(output_file)\n    else:\n        print(\"No codes found for BC\")\n        df_bc = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7b: Run Manitoba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# MANITOBA - FULL CROSSWALK\n# ============================================================================\n\nPROV_CODE = 'MB'\nprov_config = PROVINCE_CONFIGS[PROV_CODE]\n\nprint(\"=\"*70)\nprint(f\"MANITOBA CROSSWALK\")\nprint(\"=\"*70)\n\n# Check files exist\nif not PDF_FILES.get(PROV_CODE) or not REF_FILES.get(PROV_CODE):\n    print(f\"ERROR: Missing files for {PROV_CODE}\")\n    print(f\"  PDF: {PDF_FILES.get(PROV_CODE)}\")\n    print(f\"  REF: {REF_FILES.get(PROV_CODE)}\")\nelse:\n    reset_cost_tracking()\n    \n    # Load reference CSV\n    df_ref = pd.read_csv(REF_FILES[PROV_CODE])\n    df_ref = df_ref.sort_values('page_start').reset_index(drop=True)\n    print(f\"Loaded {len(df_ref)} section entries from reference CSV\")\n    \n    # Load PDF pages\n    print(f\"\\nLoading PDF: {PDF_FILES[PROV_CODE]}\")\n    pdf_pages, total_pages = load_pdf_pages(PDF_FILES[PROV_CODE])\n    print(f\"Loaded {len(pdf_pages)} pages\")\n    \n    # Build section chunks (Level 1)\n    section_chunks = build_section_chunks_level1(df_ref, pdf_pages, total_pages, prov_config)\n    print(f\"\\nCreated {len(section_chunks)} section chunks\")\n    \n    # --- PHASE 1: Extract codes ---\n    print(f\"\\n{'='*70}\")\n    print(\"PHASE 1: EXTRACTING CODES\")\n    print(\"=\"*70)\n    \n    all_results_mb = []\n    code_chunks_mb = {}\n    \n    for section_key, section_info in tqdm(section_chunks.items(), desc=\"Processing sections\"):\n        char_count = section_info['char_count']\n        max_tokens = get_dynamic_max_tokens(char_count)\n        \n        prompt = build_phase1_prompt(section_key, section_info, PROV_CODE, prov_config)\n        \n        try:\n            resp = client.chat.completions.create(\n                model=\"gpt-5.1-2025-11-13\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_completion_tokens=max_tokens\n            )\n            track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n            \n            content = resp.choices[0].message.content\n            match = re.search(r'\\{[\\s\\S]*\\}', content)\n            \n            if match:\n                result = json.loads(match.group())\n                if result.get('found'):\n                    rows = process_phase1_result(result, section_key, section_info, PROV_CODE, code_chunks_mb)\n                    all_results_mb.extend(rows)\n                    print(f\"  [{section_key[:40]}] -> {len(rows)} codes\")\n        except Exception as e:\n            print(f\"  [{section_key[:40]}] ERROR: {e}\")\n    \n    print(f\"\\nPhase 1 complete: {len(all_results_mb)} codes found\")\n    \n    # --- PHASE 2: Extract attributes ---\n    if len(all_results_mb) > 0:\n        print(f\"\\n{'='*70}\")\n        print(\"PHASE 2: EXTRACTING ATTRIBUTES\")\n        print(\"=\"*70)\n        \n        rules_text = extract_rules_text(PDF_FILES[PROV_CODE], prov_config['rules_pages'])\n        print(f\"Loaded rules text: {len(rules_text):,} chars\")\n        \n        # Dynamic token limit based on rules size\n        phase2_tokens = get_phase2_max_tokens(len(rules_text))\n        print(f\"Phase 2 max_completion_tokens: {phase2_tokens}\")\n        \n        phase2_results = []\n        for code_info in tqdm(all_results_mb, desc=\"Extracting attributes\"):\n            unique_key = code_info.get('_unique_key', '')\n            chunk_text = code_chunks_mb.get(unique_key, '')\n            \n            prompt = build_phase2_prompt(code_info, chunk_text, rules_text, PROV_CODE)\n            \n            try:\n                resp = client.chat.completions.create(\n                    model=\"gpt-5.1-2025-11-13\",\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    temperature=0.1,\n                    max_completion_tokens=phase2_tokens\n                )\n                track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n                \n                content = resp.choices[0].message.content\n                match = re.search(r'\\{[\\s\\S]*\\}', content)\n                \n                if match:\n                    attrs = json.loads(match.group())\n                    if attrs.get('same_day_exclusions') and isinstance(attrs['same_day_exclusions'], list):\n                        attrs['same_day_exclusions'] = ', '.join(attrs['same_day_exclusions'])\n                    phase2_results.append({'_unique_key': unique_key, **attrs})\n                else:\n                    phase2_results.append({'_unique_key': unique_key})\n            except Exception as e:\n                phase2_results.append({'_unique_key': unique_key})\n        \n        # Combine Phase 1 + Phase 2\n        df_phase1 = pd.DataFrame(all_results_mb)\n        df_phase2 = pd.DataFrame(phase2_results)\n        df_mb = df_phase1.merge(df_phase2, on='_unique_key', how='left')\n        df_mb = df_mb.drop(columns=['_unique_key'])\n        \n        # Save\n        output_file = f\"3.02_MB_Alberta_Complete.xlsx\"\n        df_mb.to_excel(output_file, index=False)\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"MB COMPLETE\")\n        print(\"=\"*70)\n        print(f\"Total codes: {len(df_mb)}\")\n        print(f\"  - PRIMARY: {len(df_mb[df_mb['Type'] == 'PRIMARY'])}\")\n        print(f\"  - ADD-ON: {len(df_mb[df_mb['Type'] == 'ADD-ON'])}\")\n        print(f\"API calls: {total_calls} | Cost: ${total_cost:.2f}\")\n        print(f\"\\nSaved: {output_file}\")\n        \n        files.download(output_file)\n    else:\n        print(\"No codes found for MB\")\n        df_mb = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7c: Run Ontario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ONTARIO - FULL CROSSWALK\n# ============================================================================\n\nPROV_CODE = 'ON'\nprov_config = PROVINCE_CONFIGS[PROV_CODE]\n\nprint(\"=\"*70)\nprint(f\"ONTARIO CROSSWALK\")\nprint(\"=\"*70)\n\n# Check files exist\nif not PDF_FILES.get(PROV_CODE) or not REF_FILES.get(PROV_CODE):\n    print(f\"ERROR: Missing files for {PROV_CODE}\")\n    print(f\"  PDF: {PDF_FILES.get(PROV_CODE)}\")\n    print(f\"  REF: {REF_FILES.get(PROV_CODE)}\")\nelse:\n    reset_cost_tracking()\n    \n    # Load reference CSV\n    df_ref = pd.read_csv(REF_FILES[PROV_CODE])\n    df_ref = df_ref.sort_values('page_start').reset_index(drop=True)\n    print(f\"Loaded {len(df_ref)} section entries from reference CSV\")\n    \n    # Load PDF pages\n    print(f\"\\nLoading PDF: {PDF_FILES[PROV_CODE]}\")\n    pdf_pages, total_pages = load_pdf_pages(PDF_FILES[PROV_CODE])\n    print(f\"Loaded {len(pdf_pages)} pages\")\n    \n    # Build section chunks (Level 2 for Ontario)\n    section_chunks = build_section_chunks_level2(df_ref, pdf_pages, total_pages, prov_config)\n    print(f\"\\nCreated {len(section_chunks)} section chunks\")\n    \n    # --- PHASE 1: Extract codes ---\n    print(f\"\\n{'='*70}\")\n    print(\"PHASE 1: EXTRACTING CODES\")\n    print(\"=\"*70)\n    \n    all_results_on = []\n    code_chunks_on = {}\n    \n    for section_key, section_info in tqdm(section_chunks.items(), desc=\"Processing sections\"):\n        char_count = section_info['char_count']\n        max_tokens = get_dynamic_max_tokens(char_count)\n        \n        prompt = build_phase1_prompt(section_key, section_info, PROV_CODE, prov_config)\n        \n        try:\n            resp = client.chat.completions.create(\n                model=\"gpt-5.1-2025-11-13\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_completion_tokens=max_tokens\n            )\n            track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n            \n            content = resp.choices[0].message.content\n            match = re.search(r'\\{[\\s\\S]*\\}', content)\n            \n            if match:\n                result = json.loads(match.group())\n                if result.get('found'):\n                    rows = process_phase1_result(result, section_key, section_info, PROV_CODE, code_chunks_on)\n                    all_results_on.extend(rows)\n                    print(f\"  [{section_key[:40]}] -> {len(rows)} codes\")\n        except Exception as e:\n            print(f\"  [{section_key[:40]}] ERROR: {e}\")\n    \n    print(f\"\\nPhase 1 complete: {len(all_results_on)} codes found\")\n    \n    # --- PHASE 2: Extract attributes ---\n    if len(all_results_on) > 0:\n        print(f\"\\n{'='*70}\")\n        print(\"PHASE 2: EXTRACTING ATTRIBUTES\")\n        print(\"=\"*70)\n        \n        rules_text = extract_rules_text(PDF_FILES[PROV_CODE], prov_config['rules_pages'])\n        print(f\"Loaded rules text: {len(rules_text):,} chars\")\n        \n        # Dynamic token limit based on rules size\n        phase2_tokens = get_phase2_max_tokens(len(rules_text))\n        print(f\"Phase 2 max_completion_tokens: {phase2_tokens}\")\n        \n        phase2_results = []\n        for code_info in tqdm(all_results_on, desc=\"Extracting attributes\"):\n            unique_key = code_info.get('_unique_key', '')\n            chunk_text = code_chunks_on.get(unique_key, '')\n            \n            prompt = build_phase2_prompt(code_info, chunk_text, rules_text, PROV_CODE)\n            \n            try:\n                resp = client.chat.completions.create(\n                    model=\"gpt-5.1-2025-11-13\",\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    temperature=0.1,\n                    max_completion_tokens=phase2_tokens\n                )\n                track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n                \n                content = resp.choices[0].message.content\n                match = re.search(r'\\{[\\s\\S]*\\}', content)\n                \n                if match:\n                    attrs = json.loads(match.group())\n                    if attrs.get('same_day_exclusions') and isinstance(attrs['same_day_exclusions'], list):\n                        attrs['same_day_exclusions'] = ', '.join(attrs['same_day_exclusions'])\n                    phase2_results.append({'_unique_key': unique_key, **attrs})\n                else:\n                    phase2_results.append({'_unique_key': unique_key})\n            except Exception as e:\n                phase2_results.append({'_unique_key': unique_key})\n        \n        # Combine Phase 1 + Phase 2\n        df_phase1 = pd.DataFrame(all_results_on)\n        df_phase2 = pd.DataFrame(phase2_results)\n        df_on = df_phase1.merge(df_phase2, on='_unique_key', how='left')\n        df_on = df_on.drop(columns=['_unique_key'])\n        \n        # Save\n        output_file = f\"3.02_ON_Alberta_Complete.xlsx\"\n        df_on.to_excel(output_file, index=False)\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"ON COMPLETE\")\n        print(\"=\"*70)\n        print(f\"Total codes: {len(df_on)}\")\n        print(f\"  - PRIMARY: {len(df_on[df_on['Type'] == 'PRIMARY'])}\")\n        print(f\"  - ADD-ON: {len(df_on[df_on['Type'] == 'ADD-ON'])}\")\n        print(f\"\\nBy Fee Type:\")\n        for ft in df_on['Fee_Type'].unique():\n            print(f\"  - {ft}: {len(df_on[df_on['Fee_Type'] == ft])}\")\n        print(f\"\\nBy Setting:\")\n        for s in df_on['Setting'].unique():\n            print(f\"  - {s}: {len(df_on[df_on['Setting'] == s])}\")\n        print(f\"\\nAPI calls: {total_calls} | Cost: ${total_cost:.2f}\")\n        print(f\"\\nSaved: {output_file}\")\n        \n        files.download(output_file)\n    else:\n        print(\"No codes found for ON\")\n        df_on = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7d: Run Saskatchewan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SASKATCHEWAN - FULL CROSSWALK\n# ============================================================================\n\nPROV_CODE = 'SK'\nprov_config = PROVINCE_CONFIGS[PROV_CODE]\n\nprint(\"=\"*70)\nprint(f\"SASKATCHEWAN CROSSWALK\")\nprint(\"=\"*70)\n\n# Check files exist\nif not PDF_FILES.get(PROV_CODE) or not REF_FILES.get(PROV_CODE):\n    print(f\"ERROR: Missing files for {PROV_CODE}\")\n    print(f\"  PDF: {PDF_FILES.get(PROV_CODE)}\")\n    print(f\"  REF: {REF_FILES.get(PROV_CODE)}\")\nelse:\n    reset_cost_tracking()\n    \n    # Load reference CSV\n    df_ref = pd.read_csv(REF_FILES[PROV_CODE])\n    df_ref = df_ref.sort_values('page_start').reset_index(drop=True)\n    print(f\"Loaded {len(df_ref)} section entries from reference CSV\")\n    \n    # Load PDF pages\n    print(f\"\\nLoading PDF: {PDF_FILES[PROV_CODE]}\")\n    pdf_pages, total_pages = load_pdf_pages(PDF_FILES[PROV_CODE])\n    print(f\"Loaded {len(pdf_pages)} pages\")\n    \n    # Build section chunks (Level 1)\n    section_chunks = build_section_chunks_level1(df_ref, pdf_pages, total_pages, prov_config)\n    print(f\"\\nCreated {len(section_chunks)} section chunks\")\n    \n    # --- PHASE 1: Extract codes ---\n    print(f\"\\n{'='*70}\")\n    print(\"PHASE 1: EXTRACTING CODES\")\n    print(\"=\"*70)\n    \n    all_results_sk = []\n    code_chunks_sk = {}\n    \n    for section_key, section_info in tqdm(section_chunks.items(), desc=\"Processing sections\"):\n        char_count = section_info['char_count']\n        max_tokens = get_dynamic_max_tokens(char_count)\n        \n        prompt = build_phase1_prompt(section_key, section_info, PROV_CODE, prov_config)\n        \n        try:\n            resp = client.chat.completions.create(\n                model=\"gpt-5.1-2025-11-13\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_completion_tokens=max_tokens\n            )\n            track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n            \n            content = resp.choices[0].message.content\n            match = re.search(r'\\{[\\s\\S]*\\}', content)\n            \n            if match:\n                result = json.loads(match.group())\n                if result.get('found'):\n                    rows = process_phase1_result(result, section_key, section_info, PROV_CODE, code_chunks_sk)\n                    all_results_sk.extend(rows)\n                    print(f\"  [{section_key[:40]}] -> {len(rows)} codes\")\n        except Exception as e:\n            print(f\"  [{section_key[:40]}] ERROR: {e}\")\n    \n    print(f\"\\nPhase 1 complete: {len(all_results_sk)} codes found\")\n    \n    # --- PHASE 2: Extract attributes ---\n    if len(all_results_sk) > 0:\n        print(f\"\\n{'='*70}\")\n        print(\"PHASE 2: EXTRACTING ATTRIBUTES\")\n        print(\"=\"*70)\n        \n        rules_text = extract_rules_text(PDF_FILES[PROV_CODE], prov_config['rules_pages'])\n        print(f\"Loaded rules text: {len(rules_text):,} chars\")\n        \n        # Dynamic token limit based on rules size\n        phase2_tokens = get_phase2_max_tokens(len(rules_text))\n        print(f\"Phase 2 max_completion_tokens: {phase2_tokens}\")\n        \n        phase2_results = []\n        for code_info in tqdm(all_results_sk, desc=\"Extracting attributes\"):\n            unique_key = code_info.get('_unique_key', '')\n            chunk_text = code_chunks_sk.get(unique_key, '')\n            \n            prompt = build_phase2_prompt(code_info, chunk_text, rules_text, PROV_CODE)\n            \n            try:\n                resp = client.chat.completions.create(\n                    model=\"gpt-5.1-2025-11-13\",\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    temperature=0.1,\n                    max_completion_tokens=phase2_tokens\n                )\n                track_cost(resp.usage.prompt_tokens, resp.usage.completion_tokens)\n                \n                content = resp.choices[0].message.content\n                match = re.search(r'\\{[\\s\\S]*\\}', content)\n                \n                if match:\n                    attrs = json.loads(match.group())\n                    if attrs.get('same_day_exclusions') and isinstance(attrs['same_day_exclusions'], list):\n                        attrs['same_day_exclusions'] = ', '.join(attrs['same_day_exclusions'])\n                    phase2_results.append({'_unique_key': unique_key, **attrs})\n                else:\n                    phase2_results.append({'_unique_key': unique_key})\n            except Exception as e:\n                phase2_results.append({'_unique_key': unique_key})\n        \n        # Combine Phase 1 + Phase 2\n        df_phase1 = pd.DataFrame(all_results_sk)\n        df_phase2 = pd.DataFrame(phase2_results)\n        df_sk = df_phase1.merge(df_phase2, on='_unique_key', how='left')\n        df_sk = df_sk.drop(columns=['_unique_key'])\n        \n        # Save\n        output_file = f\"3.02_SK_Alberta_Complete.xlsx\"\n        df_sk.to_excel(output_file, index=False)\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"SK COMPLETE\")\n        print(\"=\"*70)\n        print(f\"Total codes: {len(df_sk)}\")\n        print(f\"  - PRIMARY: {len(df_sk[df_sk['Type'] == 'PRIMARY'])}\")\n        print(f\"  - ADD-ON: {len(df_sk[df_sk['Type'] == 'ADD-ON'])}\")\n        print(f\"\\nBy Fee Type:\")\n        for ft in df_sk['Fee_Type'].unique():\n            print(f\"  - {ft}: {len(df_sk[df_sk['Fee_Type'] == ft])}\")\n        print(f\"\\nAge Premium Applies: {df_sk['Age_Premium_Applies'].sum()} codes\")\n        print(f\"\\nAPI calls: {total_calls} | Cost: ${total_cost:.2f}\")\n        print(f\"\\nSaved: {output_file}\")\n        \n        files.download(output_file)\n    else:\n        print(\"No codes found for SK\")\n        df_sk = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 7: Combine & Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Combine All Provinces & Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# COMBINE ALL PROVINCES & FINAL SUMMARY\n# ============================================================================\n\nprint(\"=\"*70)\nprint(\"COMBINING ALL PROVINCE RESULTS\")\nprint(\"=\"*70)\n\n# Collect all province DataFrames\nall_dfs = []\nprovince_stats = {}\n\n# Check each province\nfor prov_code in ['BC', 'MB', 'ON', 'SK']:\n    df_name = f\"df_{prov_code.lower()}\"\n    if df_name in dir() and len(eval(df_name)) > 0:\n        df = eval(df_name)\n        all_dfs.append(df)\n        province_stats[prov_code] = {\n            'total': len(df),\n            'primary': len(df[df['Type'] == 'PRIMARY']),\n            'addon': len(df[df['Type'] == 'ADD-ON'])\n        }\n        print(f\"  {prov_code}: {len(df)} codes\")\n    else:\n        print(f\"  {prov_code}: No data (skipped or not run)\")\n\nif len(all_dfs) > 0:\n    # Combine all\n    df_combined = pd.concat(all_dfs, ignore_index=True)\n    \n    # Fill missing columns with empty values\n    all_columns = set()\n    for df in all_dfs:\n        all_columns.update(df.columns)\n    \n    for col in all_columns:\n        if col not in df_combined.columns:\n            df_combined[col] = ''\n    \n    # Save combined file\n    ab_code = ALBERTA_CODE_CONFIG['code'].replace('.', '_')\n    combined_file = f\"3.03_All_Province_{ab_code}_Complete.xlsx\"\n    df_combined.to_excel(combined_file, index=False)\n    \n    print(f\"\\n{'='*70}\")\n    print(\"FINAL SUMMARY\")\n    print(\"=\"*70)\n    print(f\"\\nAlberta Code: {ALBERTA_CODE_CONFIG['code']} - {ALBERTA_CODE_CONFIG['description']}\")\n    print(f\"\\nTotal codes across all provinces: {len(df_combined)}\")\n    \n    print(f\"\\n--- BY PROVINCE ---\")\n    for prov, stats in province_stats.items():\n        print(f\"  {prov}: {stats['total']:3} codes ({stats['primary']} primary, {stats['addon']} add-on)\")\n    \n    print(f\"\\n--- BY TYPE ---\")\n    print(f\"  PRIMARY: {len(df_combined[df_combined['Type'] == 'PRIMARY'])}\")\n    print(f\"  ADD-ON: {len(df_combined[df_combined['Type'] == 'ADD-ON'])}\")\n    \n    print(f\"\\n--- OUTPUT FILES ---\")\n    for prov_code in province_stats.keys():\n        print(f\"  3.02_{prov_code}_Alberta_Complete.xlsx\")\n    print(f\"  {combined_file} (combined)\")\n    \n    print(f\"\\n{'='*70}\")\n    print(\"ALL PROVINCES COMPLETE\")\n    print(\"=\"*70)\n    \n    # Download combined file\n    files.download(combined_file)\nelse:\n    print(\"\\nNo province data available to combine.\")\n    print(\"Run the individual province cells (7a-7d) first.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}